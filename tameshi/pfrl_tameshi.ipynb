{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "choice-breathing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pfrl\n",
    "import torch\n",
    "import torch.nn\n",
    "import gym\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "laughing-steps",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation space: Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)\n",
      "action space: Discrete(2)\n",
      "initial observation: [ 0.04896488  0.04308479 -0.03475799 -0.04262517]\n",
      "next observation: [ 0.04982658 -0.15152195 -0.0356105   0.23889183]\n",
      "reward: 1.0\n",
      "done: False\n",
      "info: {}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "print('observation space:', env.observation_space)\n",
    "print('action space:', env.action_space)\n",
    "\n",
    "obs = env.reset()\n",
    "print('initial observation:', obs)\n",
    "\n",
    "action = env.action_space.sample()\n",
    "obs, r, done, info = env.step(action)\n",
    "print('next observation:', obs)\n",
    "print('reward:', r)\n",
    "print('done:', done)\n",
    "print('info:', info)\n",
    "\n",
    "# Uncomment to open a GUI window rendering the current state of the environment\n",
    "# env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "indonesian-heath",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QFunction(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, obs_size, n_actions):\n",
    "        super().__init__()\n",
    "        self.l1 = torch.nn.Linear(obs_size, 50)\n",
    "        self.l2 = torch.nn.Linear(50, 50)\n",
    "        self.l3 = torch.nn.Linear(50, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = x\n",
    "        h = torch.nn.functional.relu(self.l1(h))\n",
    "        h = torch.nn.functional.relu(self.l2(h))\n",
    "        h = self.l3(h)\n",
    "        return pfrl.action_value.DiscreteActionValue(h)\n",
    "\n",
    "obs_size = env.observation_space.low.size\n",
    "n_actions = env.action_space.n\n",
    "q_func = QFunction(obs_size, n_actions)\n",
    "\n",
    "\n",
    "# Use Adam to optimize q_func. eps=1e-2 is for stability.\n",
    "optimizer = torch.optim.Adam(q_func.parameters(), eps=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "marine-funds",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the discount factor that discounts future rewards.\n",
    "gamma = 0.9\n",
    "\n",
    "# Use epsilon-greedy for exploration\n",
    "explorer = pfrl.explorers.ConstantEpsilonGreedy(\n",
    "    epsilon=0.3, random_action_func=env.action_space.sample)\n",
    "\n",
    "# DQN uses Experience Replay.\n",
    "# Specify a replay buffer and its capacity.\n",
    "replay_buffer = pfrl.replay_buffers.ReplayBuffer(capacity=10 ** 6)\n",
    "\n",
    "# Since observations from CartPole-v0 is numpy.float64 while\n",
    "# As PyTorch only accepts numpy.float32 by default, specify\n",
    "# a converter as a feature extractor function phi.\n",
    "phi = lambda x: x.astype(numpy.float32, copy=False)\n",
    "\n",
    "# Set the device id to use GPU. To use CPU only, set it to -1.\n",
    "gpu = -1\n",
    "\n",
    "# Now create an agent that will interact with the environment.\n",
    "agent = pfrl.agents.DoubleDQN(\n",
    "    q_func,\n",
    "    optimizer,\n",
    "    replay_buffer,\n",
    "    gamma,\n",
    "    explorer,\n",
    "    replay_start_size=500,\n",
    "    update_interval=1,\n",
    "    target_update_interval=100,\n",
    "    phi=phi,\n",
    "    gpu=gpu,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cubic-walnut",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 10 R: 10.0\n",
      "episode: 20 R: 10.0\n",
      "episode: 30 R: 12.0\n",
      "episode: 40 R: 12.0\n",
      "episode: 50 R: 9.0\n",
      "statistics: [('average_q', 1.0142096), ('average_loss', 0.15402821278096074), ('cumulative_steps', 571), ('n_updates', 72), ('rlen', 571)]\n",
      "episode: 60 R: 13.0\n",
      "episode: 70 R: 13.0\n",
      "episode: 80 R: 37.0\n",
      "episode: 90 R: 12.0\n",
      "episode: 100 R: 11.0\n",
      "statistics: [('average_q', 5.2103605), ('average_loss', 0.23780513466335834), ('cumulative_steps', 1277), ('n_updates', 778), ('rlen', 1277)]\n",
      "episode: 110 R: 9.0\n",
      "episode: 120 R: 11.0\n",
      "episode: 130 R: 16.0\n",
      "episode: 140 R: 14.0\n",
      "episode: 150 R: 47.0\n",
      "statistics: [('average_q', 8.236747), ('average_loss', 0.32066360825207085), ('cumulative_steps', 2240), ('n_updates', 1741), ('rlen', 2240)]\n",
      "episode: 160 R: 60.0\n",
      "episode: 170 R: 65.0\n",
      "episode: 180 R: 200.0\n",
      "episode: 190 R: 200.0\n",
      "episode: 200 R: 151.0\n",
      "statistics: [('average_q', 9.92435), ('average_loss', 0.10864238673937507), ('cumulative_steps', 9134), ('n_updates', 8635), ('rlen', 9134)]\n",
      "episode: 210 R: 70.0\n",
      "episode: 220 R: 200.0\n",
      "episode: 230 R: 200.0\n",
      "episode: 240 R: 177.0\n",
      "episode: 250 R: 200.0\n",
      "statistics: [('average_q', 10.097386), ('average_loss', 0.09339277185034006), ('cumulative_steps', 18575), ('n_updates', 18076), ('rlen', 18575)]\n",
      "episode: 260 R: 200.0\n",
      "episode: 270 R: 200.0\n",
      "episode: 280 R: 134.0\n",
      "episode: 290 R: 176.0\n",
      "episode: 300 R: 119.0\n",
      "statistics: [('average_q', 10.070279), ('average_loss', 0.05747211046051234), ('cumulative_steps', 27526), ('n_updates', 27027), ('rlen', 27526)]\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_episodes = 300\n",
    "max_episode_len = 200\n",
    "for i in range(1, n_episodes + 1):\n",
    "    obs = env.reset()\n",
    "    R = 0  # return (sum of rewards)\n",
    "    t = 0  # time step\n",
    "    while True:\n",
    "        # Uncomment to watch the behavior in a GUI window\n",
    "        # env.render()\n",
    "        action = agent.act(obs)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        R += reward\n",
    "        t += 1\n",
    "        reset = t == max_episode_len\n",
    "        agent.observe(obs, reward, done, reset)\n",
    "        if done or reset:\n",
    "            break\n",
    "    if i % 10 == 0:\n",
    "        print('episode:', i, 'R:', R)\n",
    "    if i % 50 == 0:\n",
    "        print('statistics:', agent.get_statistics())\n",
    "print('Finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "searching-dakota",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation episode: 0 R: 200.0\n",
      "evaluation episode: 1 R: 199.0\n",
      "evaluation episode: 2 R: 200.0\n",
      "evaluation episode: 3 R: 200.0\n",
      "evaluation episode: 4 R: 200.0\n",
      "evaluation episode: 5 R: 200.0\n",
      "evaluation episode: 6 R: 200.0\n",
      "evaluation episode: 7 R: 200.0\n",
      "evaluation episode: 8 R: 200.0\n",
      "evaluation episode: 9 R: 200.0\n"
     ]
    }
   ],
   "source": [
    "with agent.eval_mode():\n",
    "    for i in range(10):\n",
    "        obs = env.reset()\n",
    "        R = 0\n",
    "        t = 0\n",
    "        while True:\n",
    "            # Uncomment to watch the behavior in a GUI window\n",
    "            # env.render()\n",
    "            action = agent.act(obs)\n",
    "            obs, r, done, _ = env.step(action)\n",
    "            R += r\n",
    "            t += 1\n",
    "            reset = t == 200\n",
    "            agent.observe(obs, r, done, reset)\n",
    "            if done or reset:\n",
    "                break\n",
    "        print('evaluation episode:', i, 'R:', R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informal-sellers",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
