{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "palestinian-theorem",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要なライブラリのインポート．\n",
    "from abc import ABC, abstractmethod\n",
    "import os\n",
    "import glob\n",
    "from collections import deque\n",
    "from time import time\n",
    "from datetime import timedelta\n",
    "import pickle\n",
    "from base64 import b64encode\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions import Normal\n",
    "import torch.nn.functional as F\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "\n",
    "import robo_gym\n",
    "from robo_gym.wrappers.exception_handling import ExceptionHandling\n",
    "\n",
    "# Gymの警告を一部無視する．\n",
    "gym.logger.set_level(40)\n",
    "# matplotlibをColab上で描画するためのコマンド．\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "chubby-character",
   "metadata": {},
   "outputs": [],
   "source": [
    "def atanh(x):\n",
    "    \"\"\" tanh の逆関数． \"\"\"\n",
    "    return 0.5 * (torch.log(1 + x + 1e-6) - torch.log(1 - x + 1e-6))\n",
    "\n",
    "\n",
    "def evaluate_lop_pi(means, log_stds, actions):\n",
    "    \"\"\" 平均(mean)，標準偏差の対数(log_stds)でパラメータ化した方策における，行動(actions)の確率密度の対数を計算する． \"\"\"\n",
    "    noises = (atanh(actions) - means) / (log_stds.exp() + 1e-8)\n",
    "    return calculate_log_pi(log_stds, noises, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "played-costs",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_log_pi(log_stds, noises, actions):\n",
    "    \"\"\" 確率論的な行動の確率密度を返す． \"\"\"\n",
    "\n",
    "    # NOTE: 入力はすべて (batch_size, |A|) となっているので，この関数では　batch_size　分の確率密度の対数 \\log \\pi(a|s) を\n",
    "    # それぞれ独立に計算し (batch_size, 1) で返します．\n",
    "\n",
    "    # ガウス分布 `N(0, stds * I)` における `noises * stds` の確率密度の対数(= \\log \\pi(u|a))を計算する．\n",
    "    stds = log_stds.exp()\n",
    "    gaussian_log_probs = Normal(torch.zeros_like(stds), stds).log_prob(stds * noises).sum(dim=-1, keepdim=True)\n",
    "\n",
    "    # NOTE: gaussian_log_probs には (batch_size, 1) で表された確率密度の対数 \\log p(u|s) が入っています．\n",
    "\n",
    "    log_pis = gaussian_log_probs - torch.log(1 - actions**2 + 1e-6).sum(dim=-1, keepdim=True)\n",
    "\n",
    "    return log_pis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "intelligent-seattle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reparameterize(means, log_stds):\n",
    "    \"\"\" Reparameterization Trickを用いて，確率論的な行動とその確率密度を返す． \"\"\"\n",
    "\n",
    "    # 標準偏差．\n",
    "    stds = log_stds.exp()\n",
    "\n",
    "    noises = torch.randn_like(means)\n",
    "    actions = torch.tanh(means+noises*stds)\n",
    "\n",
    "    # 確率論的な行動の確率密度の対数を計算する．\n",
    "    log_pis = calculate_log_pi(log_stds, noises, actions)\n",
    "\n",
    "    return actions, log_pis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "australian-partition",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_state_dim(state):\n",
    "    return np.concatenate([state['agent_pose'], state['occupancy_grid']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "attended-archive",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SACActor(nn.Module):\n",
    "\n",
    "    def __init__(self, state_shape, action_shape):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_shape[0], 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 2 * action_shape[0]),\n",
    "        )\n",
    "\n",
    "    def forward(self, states):\n",
    "        means, log_stds = self.net(states).chunk(2, dim=-1)\n",
    "        return torch.tanh(means)\n",
    "\n",
    "    def sample(self, states):\n",
    "        means, log_stds = self.net(states).chunk(2, dim=-1)\n",
    "        log_stds = log_stds.clamp(-20, 2)\n",
    "        return reparameterize(means, log_stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "imported-overview",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SACCritic(nn.Module):\n",
    "\n",
    "    def __init__(self, state_shape, action_shape):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net1 = nn.Sequential(\n",
    "            nn.Linear(state_shape[0] + action_shape[0], 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "        self.net2 = nn.Sequential(\n",
    "            nn.Linear(state_shape[0] + action_shape[0], 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, states, actions):\n",
    "        sa = torch.cat([states, actions], dim=-1)\n",
    "        q1 = self.net1(sa)\n",
    "        q2 = self.net2(sa)\n",
    "        return q1, q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "unlikely-infrastructure",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "\n",
    "    def __init__(self, buffer_size, state_shape, action_shape, device):\n",
    "        # 次にデータを挿入するインデックス．\n",
    "        self._p = 0\n",
    "        # データ数．\n",
    "        self._n = 0\n",
    "        # リプレイバッファのサイズ．\n",
    "        self.buffer_size = buffer_size\n",
    "\n",
    "        # GPU上に保存するデータ．\n",
    "        self.states = torch.empty((buffer_size, *state_shape), dtype=torch.float, device=device)\n",
    "        self.actions = torch.empty((buffer_size, *action_shape), dtype=torch.float, device=device)\n",
    "        self.rewards = torch.empty((buffer_size, 1), dtype=torch.float, device=device)\n",
    "        self.dones = torch.empty((buffer_size, 1), dtype=torch.float, device=device)\n",
    "        self.next_states = torch.empty((buffer_size, *state_shape), dtype=torch.float, device=device)\n",
    "\n",
    "    def append(self, state, action, reward, done, next_state):\n",
    "        self.states[self._p].copy_(torch.from_numpy(state))\n",
    "        self.actions[self._p].copy_(torch.from_numpy(action))\n",
    "        self.rewards[self._p] = float(reward)\n",
    "        self.dones[self._p] = float(done)\n",
    "        self.next_states[self._p].copy_(torch.from_numpy(next_state))\n",
    "\n",
    "        self._p = (self._p + 1) % self.buffer_size\n",
    "        self._n = min(self._n + 1, self.buffer_size)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        idxes = np.random.randint(low=0, high=self._n, size=batch_size)\n",
    "        return (\n",
    "            self.states[idxes],\n",
    "            self.actions[idxes],\n",
    "            self.rewards[idxes],\n",
    "            self.dones[idxes],\n",
    "            self.next_states[idxes]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "velvet-fields",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Algorithm(ABC):\n",
    "\n",
    "    def explore(self, state):\n",
    "        \"\"\" 確率論的な行動と，その行動の確率密度の対数 \\log(\\pi(a|s)) を返す． \"\"\"\n",
    "        state = torch.tensor(state, dtype=torch.float, device=self.device).unsqueeze_(0)\n",
    "        with torch.no_grad():\n",
    "            action, log_pi = self.actor.sample(state)\n",
    "        return action.cpu().numpy()[0], log_pi.item()\n",
    "\n",
    "    def exploit(self, state):\n",
    "        \"\"\" 決定論的な行動を返す． \"\"\"\n",
    "        state = torch.tensor(state, dtype=torch.float, device=self.device).unsqueeze_(0)\n",
    "        with torch.no_grad():\n",
    "            action = self.actor(state)\n",
    "        return action.cpu().numpy()[0]\n",
    "\n",
    "    @abstractmethod\n",
    "    def is_update(self, steps):\n",
    "        \"\"\" 現在のトータルのステップ数(steps)を受け取り，アルゴリズムを学習するか否かを返す． \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def step(self, env, state, t, steps):\n",
    "        \"\"\" 環境(env)，現在の状態(state)，現在のエピソードのステップ数(t)，今までのトータルのステップ数(steps)を\n",
    "            受け取り，リプレイバッファへの保存などの処理を行い，状態・エピソードのステップ数を更新する．\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def update(self):\n",
    "        \"\"\" 1回分の学習を行う． \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "amber-prayer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init(module):\n",
    "    nn.init.orthogonal_(module.weight.data, gain=nn.init.calculate_gain('relu'))\n",
    "    nn.init.constant_(module.bias.data, 0)\n",
    "    return module\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, map_output=100, pose_output=50, map_size=128):\n",
    "        super().__init__()\n",
    "        self.map_size = map_size\n",
    "\n",
    "        self.map_conv = nn.Sequential(\n",
    "            # 128*128 -> 32*32\n",
    "            init(nn.Conv2d(1, 16, kernel_size=8, stride=4, padding=2)),\n",
    "            nn.ReLU(),\n",
    "            # 32*32 -> 15*15\n",
    "            init(nn.Conv2d(16, 32, kernel_size=4, stride=2)),\n",
    "            nn.ReLU(),\n",
    "            # 15*15 -> 13*13\n",
    "            init(nn.Conv2d(32, 32, kernel_size=3, stride=1)),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            # 32*13*13 -> map_output\n",
    "            init(nn.Linear(32*13*13, map_output)),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.pose_buf = nn.Sequential(\n",
    "            # 3 -> pose_output\n",
    "            init(nn.Linear(3, pose_output)),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, obs):\n",
    "        map_img = obs['occupancy_grid'].reshape((self.map_size, self.map_size)).T\n",
    "        map_img = torch.tensor(map_img, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "        pose = torch.tensor(obs['agent_pose'], dtype=torch.float32)\n",
    "        \n",
    "        map_out = self.map_conv(map_img).squeeze()\n",
    "        pose_out = self.pose_buf(pose).squeeze()\n",
    "        return torch.cat([map_out, pose_out]).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "optical-delta",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "\n",
    "    def __init__(self, env, env_test, algo, encoder, seed=0, num_steps=10**6, eval_interval=10**4, num_eval_episodes=3):\n",
    "\n",
    "        self.env = env\n",
    "        self.env_test = env_test\n",
    "        self.algo = algo\n",
    "        self.encoder = encoder\n",
    "\n",
    "#         # 環境の乱数シードを設定する．\n",
    "#         self.env.seed(seed)\n",
    "#         self.env_test.seed(2**31-seed)\n",
    "\n",
    "        # 平均収益を保存するための辞書．\n",
    "        self.returns = {'step': [], 'return': []}\n",
    "\n",
    "        # データ収集を行うステップ数．\n",
    "        self.num_steps = num_steps\n",
    "        # 評価の間のステップ数(インターバル)．\n",
    "        self.eval_interval = eval_interval\n",
    "        # 評価を行うエピソード数．\n",
    "        self.num_eval_episodes = num_eval_episodes\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\" num_stepsステップの間，データ収集・学習・評価を繰り返す． \"\"\"\n",
    "\n",
    "        # 学習開始の時間\n",
    "        self.start_time = time()\n",
    "        # エピソードのステップ数．\n",
    "        t = 0\n",
    "\n",
    "        # 環境を初期化する．\n",
    "        # reset SLAM and generate new initial pose\n",
    "        state = self.env.reset(new_room=False, new_agent_pose=True)\n",
    "        with torch.no_grad():\n",
    "            state = self.encoder(state)\n",
    "\n",
    "        for steps in range(1, self.num_steps + 1):\n",
    "            # 環境(self.env)，現在の状態(state)，現在のエピソードのステップ数(t)，今までのトータルのステップ数(steps)を\n",
    "            # アルゴリズムに渡し，状態・エピソードのステップ数を更新する．\n",
    "            \n",
    "            state, t = self.algo.step(self.env, state, t, steps)\n",
    "\n",
    "            # アルゴリズムが準備できていれば，1回学習を行う．\n",
    "            if self.algo.is_update(steps):\n",
    "                self.algo.update()\n",
    "\n",
    "            # 一定のインターバルで評価する．\n",
    "            if steps % self.eval_interval == 0:\n",
    "                self.evaluate(steps)\n",
    "                \n",
    "    def evaluate(self, steps):\n",
    "        \"\"\" 複数エピソード環境を動かし，平均収益を記録する． \"\"\"\n",
    "\n",
    "        returns = []\n",
    "        for _ in range(self.num_eval_episodes):\n",
    "            state = self.env_test.reset()\n",
    "            state = fix_state_dim(state)\n",
    "            done = False\n",
    "            episode_return = 0.0\n",
    "\n",
    "            while (not done):\n",
    "                action = self.algo.exploit(state)\n",
    "                print(action)\n",
    "                state, reward, done, _ = self.env_test.step(action)\n",
    "                with torch.no_grad():\n",
    "                    state = self.encoder(state)\n",
    "                episode_return += reward\n",
    "\n",
    "            returns.append(episode_return)\n",
    "\n",
    "        mean_return = np.mean(returns)\n",
    "        self.returns['step'].append(steps)\n",
    "        self.returns['return'].append(mean_return)\n",
    "\n",
    "        print(f'Num steps: {steps:<6}   '\n",
    "              f'Return: {mean_return:<5.1f}   '\n",
    "              f'Time: {self.time}')\n",
    "        \n",
    "    def plot(self):\n",
    "        \"\"\" 平均収益のグラフを描画する． \"\"\"\n",
    "        fig = plt.figure(figsize=(8, 6))\n",
    "        plt.plot(self.returns['step'], self.returns['return'])\n",
    "        plt.xlabel('Steps', fontsize=24)\n",
    "        plt.ylabel('Return', fontsize=24)\n",
    "        plt.tick_params(labelsize=18)\n",
    "        plt.title(f'{self.env.unwrapped.spec.id}', fontsize=24)\n",
    "        plt.tight_layout()\n",
    "\n",
    "    @property\n",
    "    def time(self):\n",
    "        \"\"\" 学習開始からの経過時間． \"\"\"\n",
    "        return str(timedelta(seconds=int(time() - self.start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "third-jamaica",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAC(Algorithm):\n",
    "\n",
    "    def __init__(self, state_shape, action_shape, encoder, device=torch.device('cuda'), seed=0,\n",
    "                 batch_size=256, gamma=0.99, lr_actor=3e-4, lr_critic=3e-4,\n",
    "                 replay_size=10**6, start_steps=10**4, tau=5e-3, alpha=0.2, reward_scale=1.0):\n",
    "        super().__init__()\n",
    "\n",
    "        # シードを設定する．\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "        # リプレイバッファ．\n",
    "        self.buffer = ReplayBuffer(\n",
    "            buffer_size=replay_size,\n",
    "            state_shape=state_shape,\n",
    "            action_shape=action_shape,\n",
    "            device=device,\n",
    "        )\n",
    "        \n",
    "        self.encoder = encoder\n",
    "\n",
    "        # Actor-Criticのネットワークを構築する．\n",
    "        self.actor = SACActor(\n",
    "            state_shape=state_shape,\n",
    "            action_shape=action_shape\n",
    "        ).to(device)\n",
    "        self.critic = SACCritic(\n",
    "            state_shape=state_shape,\n",
    "            action_shape=action_shape\n",
    "        ).to(device)\n",
    "        self.critic_target = SACCritic(\n",
    "            state_shape=state_shape,\n",
    "            action_shape=action_shape\n",
    "        ).to(device).eval()\n",
    "\n",
    "        # ターゲットネットワークの重みを初期化し，勾配計算を無効にする．\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "        for param in self.critic_target.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # オプティマイザ．\n",
    "        self.optim_actor = torch.optim.Adam(self.actor.parameters(), lr=lr_actor)\n",
    "        self.optim_critic = torch.optim.Adam(self.critic.parameters(), lr=lr_critic)\n",
    "\n",
    "        # その他パラメータ．\n",
    "        self.learning_steps = 0\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        self.gamma = gamma\n",
    "        self.start_steps = start_steps\n",
    "        self.tau = tau\n",
    "        self.alpha = alpha\n",
    "        self.reward_scale = reward_scale\n",
    "\n",
    "    def is_update(self, steps):\n",
    "        # 学習初期の一定期間(start_steps)は学習しない．\n",
    "        return steps >= max(self.start_steps, self.batch_size)\n",
    "\n",
    "    def step(self, env, state, t, steps):\n",
    "        t += 1\n",
    "\n",
    "        # 学習初期の一定期間(start_steps)は，ランダムに行動して多様なデータの収集を促進する．\n",
    "        if steps <= self.start_steps:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action, _ = self.explore(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        with torch.no_grad():\n",
    "            next_state = self.encoder(next_state)\n",
    "        \n",
    "        \n",
    "        # not ignore done\n",
    "\n",
    "        # リプレイバッファにデータを追加する．\n",
    "        self.buffer.append(state, action, reward, done, next_state)\n",
    "\n",
    "        # エピソードが終了した場合には，環境をリセットする．\n",
    "        if done:\n",
    "            t = 0\n",
    "            next_state = env.reset()\n",
    "            with torch.no_grad():\n",
    "                next_state = self.encoder(next_state)\n",
    "\n",
    "        return next_state, t\n",
    "\n",
    "    def update(self):\n",
    "        self.learning_steps += 1\n",
    "        states, actions, rewards, dones, next_states = self.buffer.sample(self.batch_size)\n",
    "\n",
    "        self.update_critic(states, actions, rewards, dones, next_states)\n",
    "        self.update_actor(states)\n",
    "        self.update_target()\n",
    "\n",
    "    def update_critic(self, states, actions, rewards, dones, next_states):\n",
    "        # 現在のソフト状態行動価値を計算する．\n",
    "        curr_qs1, curr_qs2 = self.critic(states, actions)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # ソフト状態価値のターゲットを計算します．\n",
    "            # target_vs = ...\n",
    "            next_actions, log_pis = self.actor.sample(next_states)\n",
    "            qs1, qs2 = self.critic_target(next_states, next_actions)\n",
    "            target_vs = torch.min(qs1, qs2) - self.alpha * log_pis\n",
    "\n",
    "        # ソフト状態行動価値のターゲットを計算します．\n",
    "        # target_qs = ...\n",
    "        target_qs = rewards * self.reward_scale + (1 - dones) * self.gamma * target_vs\n",
    "\n",
    "        loss_critic1 = (curr_qs1 - target_qs).pow_(2).mean()\n",
    "        loss_critic2 = (curr_qs2 - target_qs).pow_(2).mean()\n",
    "\n",
    "        self.optim_critic.zero_grad()\n",
    "        (loss_critic1 + loss_critic2).backward(retain_graph=False)\n",
    "        self.optim_critic.step()\n",
    "\n",
    "    def update_actor(self, states):\n",
    "        actions, log_pis = self.actor.sample(states)\n",
    "        qs1, qs2 = self.critic(states, actions)\n",
    "        loss_actor = (self.alpha * log_pis - torch.min(qs1, qs2)).mean()\n",
    "\n",
    "        self.optim_actor.zero_grad()\n",
    "        loss_actor.backward(retain_graph=False)\n",
    "        self.optim_actor.step()\n",
    "\n",
    "    def update_target(self):\n",
    "        for t, s in zip(self.critic_target.parameters(), self.critic.parameters()):\n",
    "            t.data.mul_(1.0 - self.tau)\n",
    "            t.data.add_(self.tau * s.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "incorporated-meter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting new Robot Server | Tentative 1\n",
      "<class 'server_manager_pb2.RobotServer'>\n",
      "True \n",
      "Successfully started Robot Server at localhost:44113\n",
      "Starting new Robot Server | Tentative 1\n",
      "<class 'server_manager_pb2.RobotServer'>\n",
      "True \n",
      "Successfully started Robot Server at localhost:39143\n"
     ]
    }
   ],
   "source": [
    "ENV_ID = 'CubeRoomSearchLikeContinuously-v0'\n",
    "target_machine_ip = 'localhost'\n",
    "SEED = 0\n",
    "REWARD_SCALE = 1.0\n",
    "NUM_STEPS = 5 * 10 ** 4\n",
    "EVAL_INTERVAL = 10 ** 3\n",
    "\n",
    "map_enc = 100\n",
    "pose_enc = 50\n",
    "\n",
    "env = gym.make(ENV_ID, ip=target_machine_ip, gui=True, max_episode_steps=100)\n",
    "env_test = gym.make(ENV_ID, ip=target_machine_ip, gui=True, max_episode_steps=100)\n",
    "\n",
    "encoder = Encoder(map_enc, pose_enc, env.map_size)\n",
    "\n",
    "# s_shape = (env.observation_space['agent_pose'].low.size + env.observation_space['occupancy_grid'].low.size, )\n",
    "state_shape = (map_enc+pose_enc,)\n",
    "\n",
    "algo = SAC(\n",
    "    state_shape=state_shape,\n",
    "    action_shape=env.action_space.shape,\n",
    "    seed=SEED,\n",
    "    replay_size=10**6,\n",
    "    reward_scale=REWARD_SCALE,\n",
    "    encoder=encoder,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    env=env,\n",
    "    env_test=env,\n",
    "    algo=algo,\n",
    "    seed=SEED,\n",
    "    num_steps=NUM_STEPS,\n",
    "    eval_interval=EVAL_INTERVAL,\n",
    "    encoder=encoder,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "coastal-ballet",
   "metadata": {},
   "outputs": [
    {
     "ename": "_InactiveRpcError",
     "evalue": "<_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.UNAVAILABLE\n\tdetails = \"Socket closed\"\n\tdebug_error_string = \"{\"created\":\"@1616939097.175174700\",\"description\":\"Error received from peer ipv4:127.0.0.1:44113\",\"file\":\"src/core/lib/surface/call.cc\",\"file_line\":1063,\"grpc_message\":\"Socket closed\",\"grpc_status\":14}\"\n>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-3435b262f1ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-26b2231247e1>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;31m# アルゴリズムに渡し，状態・エピソードのステップ数を更新する．\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;31m# アルゴリズムが準備できていれば，1回学習を行う．\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-58947efa692b>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, env, state, t, steps)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/robo_gym/envs/mir_nav/mir_nav.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;31m# Send action to Robot Server\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrs_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRobotServerError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"send_action\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/robo_gym_server_modules/robot_server/client.py\u001b[0m in \u001b[0;36msend_action\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msend_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrobot_server_stub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSendAction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrobot_server_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuccess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/grpc/_channel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m    921\u001b[0m         state, call, = self._blocking(request, timeout, metadata, credentials,\n\u001b[1;32m    922\u001b[0m                                       wait_for_ready, compression)\n\u001b[0;32m--> 923\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_end_unary_response_blocking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    924\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m     def with_call(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/grpc/_channel.py\u001b[0m in \u001b[0;36m_end_unary_response_blocking\u001b[0;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[1;32m    824\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 826\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0m_InactiveRpcError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    827\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.UNAVAILABLE\n\tdetails = \"Socket closed\"\n\tdebug_error_string = \"{\"created\":\"@1616939097.175174700\",\"description\":\"Error received from peer ipv4:127.0.0.1:44113\",\"file\":\"src/core/lib/surface/call.cc\",\"file_line\":1063,\"grpc_message\":\"Socket closed\",\"grpc_status\":14}\"\n>"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "premier-confidentiality",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "general-figure",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.ReplayBuffer at 0x7fdac2d285c0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.algo.buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secure-variety",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
