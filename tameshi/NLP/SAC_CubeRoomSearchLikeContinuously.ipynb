{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "palestinian-theorem",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要なライブラリのインポート．\n",
    "from abc import ABC, abstractmethod\n",
    "import os\n",
    "import glob\n",
    "from collections import deque\n",
    "from time import time\n",
    "from datetime import timedelta\n",
    "import pickle\n",
    "from base64 import b64encode\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions import Normal\n",
    "import torch.nn.functional as F\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "\n",
    "import robo_gym\n",
    "from robo_gym.wrappers.exception_handling import ExceptionHandling\n",
    "\n",
    "# Gymの警告を一部無視する．\n",
    "gym.logger.set_level(40)\n",
    "# matplotlibをColab上で描画するためのコマンド．\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chubby-character",
   "metadata": {},
   "outputs": [],
   "source": [
    "def atanh(x):\n",
    "    \"\"\" tanh の逆関数． \"\"\"\n",
    "    return 0.5 * (torch.log(1 + x + 1e-6) - torch.log(1 - x + 1e-6))\n",
    "\n",
    "\n",
    "def evaluate_lop_pi(means, log_stds, actions):\n",
    "    \"\"\" 平均(mean)，標準偏差の対数(log_stds)でパラメータ化した方策における，行動(actions)の確率密度の対数を計算する． \"\"\"\n",
    "    noises = (atanh(actions) - means) / (log_stds.exp() + 1e-8)\n",
    "    return calculate_log_pi(log_stds, noises, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "played-costs",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_log_pi(log_stds, noises, actions):\n",
    "    \"\"\" 確率論的な行動の確率密度を返す． \"\"\"\n",
    "\n",
    "    # NOTE: 入力はすべて (batch_size, |A|) となっているので，この関数では　batch_size　分の確率密度の対数 \\log \\pi(a|s) を\n",
    "    # それぞれ独立に計算し (batch_size, 1) で返します．\n",
    "\n",
    "    # ガウス分布 `N(0, stds * I)` における `noises * stds` の確率密度の対数(= \\log \\pi(u|a))を計算する．\n",
    "    stds = log_stds.exp()\n",
    "    gaussian_log_probs = Normal(torch.zeros_like(stds), stds).log_prob(stds * noises).sum(dim=-1, keepdim=True)\n",
    "\n",
    "    # NOTE: gaussian_log_probs には (batch_size, 1) で表された確率密度の対数 \\log p(u|s) が入っています．\n",
    "\n",
    "    log_pis = gaussian_log_probs - torch.log(1 - actions**2 + 1e-6).sum(dim=-1, keepdim=True)\n",
    "\n",
    "    return log_pis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intelligent-seattle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reparameterize(means, log_stds):\n",
    "    \"\"\" Reparameterization Trickを用いて，確率論的な行動とその確率密度を返す． \"\"\"\n",
    "\n",
    "    # 標準偏差．\n",
    "    stds = log_stds.exp()\n",
    "\n",
    "    noises = torch.randn_like(means)\n",
    "    actions = torch.tanh(means+noises*stds)\n",
    "\n",
    "    # 確率論的な行動の確率密度の対数を計算する．\n",
    "    log_pis = calculate_log_pi(log_stds, noises, actions)\n",
    "\n",
    "    return actions, log_pis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "australian-partition",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_state_dim(state):\n",
    "    return np.concatenate([state['agent_pose'], state['occupancy_grid']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attended-archive",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SACActor(nn.Module):\n",
    "\n",
    "    def __init__(self, state_shape, action_shape):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_shape[0], 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 2 * action_shape[0]),\n",
    "        )\n",
    "        \n",
    "        torch.nn.init.xavier_uniform_(self.net[0].weight)\n",
    "        torch.nn.init.xavier_uniform_(self.net[2].weight)\n",
    "        torch.nn.init.xavier_uniform_(self.net[4].weight, gain=1.0)\n",
    "\n",
    "    def forward(self, states):\n",
    "        means, log_stds = self.net(states).chunk(2, dim=-1)\n",
    "        return torch.tanh(means)\n",
    "\n",
    "    def sample(self, states):\n",
    "        means, log_stds = self.net(states).chunk(2, dim=-1)\n",
    "        log_stds = log_stds.clamp(-20, 2)\n",
    "        return reparameterize(means, log_stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imported-overview",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SACCritic(nn.Module):\n",
    "\n",
    "    def __init__(self, state_shape, action_shape):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net1 = nn.Sequential(\n",
    "            nn.Linear(state_shape[0] + action_shape[0], 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "        self.net2 = nn.Sequential(\n",
    "            nn.Linear(state_shape[0] + action_shape[0], 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "        \n",
    "        for i in [0,2,4]:\n",
    "            torch.nn.init.xavier_uniform_(self.net1[i].weight)\n",
    "            torch.nn.init.xavier_uniform_(self.net2[i].weight)\n",
    "\n",
    "    def forward(self, states, actions):\n",
    "        sa = torch.cat([states, actions], dim=-1)\n",
    "        q1 = self.net1(sa)\n",
    "        q2 = self.net2(sa)\n",
    "        return q1, q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlikely-infrastructure",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "\n",
    "    def __init__(self, buffer_size, state_shape, action_shape, device):\n",
    "        # 次にデータを挿入するインデックス．\n",
    "        self._p = 0\n",
    "        # データ数．\n",
    "        self._n = 0\n",
    "        # リプレイバッファのサイズ．\n",
    "        self.buffer_size = buffer_size\n",
    "\n",
    "        # GPU上に保存するデータ．\n",
    "        self.states = torch.empty((buffer_size, *state_shape), dtype=torch.float, device=device)\n",
    "        self.actions = torch.empty((buffer_size, *action_shape), dtype=torch.float, device=device)\n",
    "        self.rewards = torch.empty((buffer_size, 1), dtype=torch.float, device=device)\n",
    "        self.dones = torch.empty((buffer_size, 1), dtype=torch.float, device=device)\n",
    "        self.next_states = torch.empty((buffer_size, *state_shape), dtype=torch.float, device=device)\n",
    "\n",
    "    def append(self, state, action, reward, done, next_state):\n",
    "        self.states[self._p].copy_(torch.from_numpy(state))\n",
    "        self.actions[self._p].copy_(torch.from_numpy(action))\n",
    "        self.rewards[self._p] = float(reward)\n",
    "        self.dones[self._p] = float(done)\n",
    "        self.next_states[self._p].copy_(torch.from_numpy(next_state))\n",
    "\n",
    "        self._p = (self._p + 1) % self.buffer_size\n",
    "        self._n = min(self._n + 1, self.buffer_size)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        idxes = np.random.randint(low=0, high=self._n, size=batch_size)\n",
    "        return (\n",
    "            self.states[idxes],\n",
    "            self.actions[idxes],\n",
    "            self.rewards[idxes],\n",
    "            self.dones[idxes],\n",
    "            self.next_states[idxes]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "velvet-fields",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Algorithm(ABC):\n",
    "\n",
    "    def explore(self, state):\n",
    "        \"\"\" 確率論的な行動と，その行動の確率密度の対数 \\log(\\pi(a|s)) を返す． \"\"\"\n",
    "        state = torch.tensor(state, dtype=torch.float, device=self.device).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            action, log_pi = self.actor.sample(state)\n",
    "        return action.cpu().numpy()[0], log_pi.item()\n",
    "\n",
    "    def exploit(self, state):\n",
    "        \"\"\" 決定論的な行動を返す． \"\"\"\n",
    "        state = torch.tensor(state, dtype=torch.float, device=self.device).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            action = self.actor(state)\n",
    "        return action.cpu().numpy()[0]\n",
    "\n",
    "    @abstractmethod\n",
    "    def is_update(self, steps):\n",
    "        \"\"\" 現在のトータルのステップ数(steps)を受け取り，アルゴリズムを学習するか否かを返す． \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def step(self, env, state, t, steps):\n",
    "        \"\"\" 環境(env)，現在の状態(state)，現在のエピソードのステップ数(t)，今までのトータルのステップ数(steps)を\n",
    "            受け取り，リプレイバッファへの保存などの処理を行い，状態・エピソードのステップ数を更新する．\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def update(self):\n",
    "        \"\"\" 1回分の学習を行う． \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amber-prayer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init(module):\n",
    "    nn.init.orthogonal_(module.weight.data, gain=nn.init.calculate_gain('relu'))\n",
    "    nn.init.constant_(module.bias.data, 0)\n",
    "    return module\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, map_output=100, pose_output=50, map_size=128):\n",
    "        super().__init__()\n",
    "        self.map_size = map_size\n",
    "\n",
    "        self.map_conv = nn.Sequential(\n",
    "            # 128*128 -> 32*32\n",
    "            init(nn.Conv2d(1, 16, kernel_size=8, stride=4, padding=2)),\n",
    "            nn.ReLU(),\n",
    "            # 32*32 -> 15*15\n",
    "            init(nn.Conv2d(16, 32, kernel_size=4, stride=2)),\n",
    "            nn.ReLU(),\n",
    "            # 15*15 -> 13*13\n",
    "            init(nn.Conv2d(32, 32, kernel_size=3, stride=1)),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            # 32*13*13 -> map_output\n",
    "            init(nn.Linear(32*13*13, map_output)),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.pose_buf = nn.Sequential(\n",
    "            # 3 -> pose_output\n",
    "            init(nn.Linear(3, pose_output)),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, obs):\n",
    "        map_img = obs['occupancy_grid'].reshape((self.map_size, self.map_size)).T\n",
    "        map_img = torch.tensor(map_img, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "        pose = torch.tensor(obs['agent_pose'], dtype=torch.float32)\n",
    "        \n",
    "        map_out = self.map_conv(map_img).squeeze()\n",
    "        pose_out = self.pose_buf(pose).squeeze()\n",
    "        return torch.cat([map_out, pose_out]).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optical-delta",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "\n",
    "    def __init__(self, env, env_test, algo, encoder, seed=0, num_steps=10**6, eval_interval=10**4, num_eval_episodes=3):\n",
    "\n",
    "        self.env = env\n",
    "        self.env_test = env_test\n",
    "        self.algo = algo\n",
    "        self.encoder = encoder\n",
    "\n",
    "#         # 環境の乱数シードを設定する．\n",
    "#         self.env.seed(seed)\n",
    "#         self.env_test.seed(2**31-seed)\n",
    "\n",
    "        # 平均収益を保存するための辞書．\n",
    "        self.returns = {'step': [], 'return': []}\n",
    "\n",
    "        # データ収集を行うステップ数．\n",
    "        self.num_steps = num_steps\n",
    "        # 評価の間のステップ数(インターバル)．\n",
    "        self.eval_interval = eval_interval\n",
    "        # 評価を行うエピソード数．\n",
    "        self.num_eval_episodes = num_eval_episodes\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\" num_stepsステップの間，データ収集・学習・評価を繰り返す． \"\"\"\n",
    "\n",
    "        # 学習開始の時間\n",
    "        self.start_time = time()\n",
    "        # エピソードのステップ数．\n",
    "        t = 0\n",
    "\n",
    "        # 環境を初期化する．\n",
    "        # reset SLAM and generate new initial pose\n",
    "        state = self.env.reset(new_room=False, new_agent_pose=True)\n",
    "        with torch.no_grad():\n",
    "            state = self.encoder(state)\n",
    "\n",
    "        for steps in range(1, self.num_steps + 1):\n",
    "            # 環境(self.env)，現在の状態(state)，現在のエピソードのステップ数(t)，今までのトータルのステップ数(steps)を\n",
    "            # アルゴリズムに渡し，状態・エピソードのステップ数を更新する．\n",
    "            \n",
    "            state, t = self.algo.step(self.env, state, t, steps)\n",
    "\n",
    "            # アルゴリズムが準備できていれば，1回学習を行う．\n",
    "            if self.algo.is_update(steps):\n",
    "                self.algo.update()\n",
    "\n",
    "            # 一定のインターバルで評価する．\n",
    "            if steps % self.eval_interval == 0:\n",
    "                self.evaluate(steps)\n",
    "                \n",
    "    def evaluate(self, steps):\n",
    "        \"\"\" 複数エピソード環境を動かし，平均収益を記録する． \"\"\"\n",
    "\n",
    "        returns = []\n",
    "        for _ in range(self.num_eval_episodes):\n",
    "            state = self.env_test.reset(new_room=True, new_agent_pose=True)\n",
    "            with torch.no_grad():\n",
    "                state = self.encoder(state)\n",
    "            done = False\n",
    "            episode_return = 0.0\n",
    "\n",
    "            while (not done):\n",
    "                action = self.algo.exploit(state)\n",
    "                print(action)\n",
    "                state, reward, done, _ = self.env_test.step(action)\n",
    "                with torch.no_grad():\n",
    "                    state = self.encoder(state)\n",
    "                episode_return += reward\n",
    "\n",
    "            returns.append(episode_return)\n",
    "\n",
    "        mean_return = np.mean(returns)\n",
    "        self.returns['step'].append(steps)\n",
    "        self.returns['return'].append(mean_return)\n",
    "\n",
    "        print(f'Num steps: {steps:<6}   '\n",
    "              f'Return: {mean_return:<5.1f}   '\n",
    "              f'Time: {self.time}')\n",
    "        \n",
    "    def plot(self):\n",
    "        \"\"\" 平均収益のグラフを描画する． \"\"\"\n",
    "        fig = plt.figure(figsize=(8, 6))\n",
    "        plt.plot(self.returns['step'], self.returns['return'])\n",
    "        plt.xlabel('Steps', fontsize=24)\n",
    "        plt.ylabel('Return', fontsize=24)\n",
    "        plt.tick_params(labelsize=18)\n",
    "        plt.title(f'{self.env.unwrapped.spec.id}', fontsize=24)\n",
    "        plt.tight_layout()\n",
    "\n",
    "    @property\n",
    "    def time(self):\n",
    "        \"\"\" 学習開始からの経過時間． \"\"\"\n",
    "        return str(timedelta(seconds=int(time() - self.start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "third-jamaica",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAC(Algorithm):\n",
    "\n",
    "    def __init__(self, state_shape, action_shape, encoder, device=torch.device('cuda'), seed=0,\n",
    "                 batch_size=256, gamma=0.99, lr_actor=3e-4, lr_critic=3e-4,\n",
    "                 replay_size=10**6, start_steps=10**4, tau=5e-3, alpha=0.2, reward_scale=1.0):\n",
    "        super().__init__()\n",
    "\n",
    "        # シードを設定する．\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "        # リプレイバッファ．\n",
    "        self.buffer = ReplayBuffer(\n",
    "            buffer_size=replay_size,\n",
    "            state_shape=state_shape,\n",
    "            action_shape=action_shape,\n",
    "            device=device,\n",
    "        )\n",
    "        \n",
    "        self.encoder = encoder\n",
    "\n",
    "        # Actor-Criticのネットワークを構築する．\n",
    "        self.actor = SACActor(\n",
    "            state_shape=state_shape,\n",
    "            action_shape=action_shape\n",
    "        ).to(device)\n",
    "        self.critic = SACCritic(\n",
    "            state_shape=state_shape,\n",
    "            action_shape=action_shape\n",
    "        ).to(device)\n",
    "        self.critic_target = SACCritic(\n",
    "            state_shape=state_shape,\n",
    "            action_shape=action_shape\n",
    "        ).to(device).eval()\n",
    "\n",
    "        # ターゲットネットワークの重みを初期化し，勾配計算を無効にする．\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "        for param in self.critic_target.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # オプティマイザ．\n",
    "        self.optim_actor = torch.optim.Adam(self.actor.parameters(), lr=lr_actor)\n",
    "        self.optim_critic = torch.optim.Adam(self.critic.parameters(), lr=lr_critic)\n",
    "\n",
    "        # その他パラメータ．\n",
    "        self.learning_steps = 0\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        self.gamma = gamma\n",
    "        self.start_steps = start_steps\n",
    "        self.tau = tau\n",
    "        self.alpha = alpha\n",
    "        self.reward_scale = reward_scale\n",
    "\n",
    "    def is_update(self, steps):\n",
    "        # 学習初期の一定期間(start_steps)は学習しない．\n",
    "        return steps >= max(self.start_steps, self.batch_size)\n",
    "\n",
    "    def step(self, env, state, t, steps):\n",
    "        t += 1\n",
    "\n",
    "        # 学習初期の一定期間(start_steps)は，ランダムに行動して多様なデータの収集を促進する．\n",
    "        if steps <= self.start_steps:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action, _ = self.explore(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        with torch.no_grad():\n",
    "            next_state = self.encoder(next_state)\n",
    "        \n",
    "        \n",
    "        # not ignore done\n",
    "\n",
    "        # リプレイバッファにデータを追加する．\n",
    "        self.buffer.append(state, action, reward, done, next_state)\n",
    "\n",
    "        # エピソードが終了した場合には，環境をリセットする．\n",
    "        if done:\n",
    "            t = 0\n",
    "            next_state = env.reset()\n",
    "            with torch.no_grad():\n",
    "                next_state = self.encoder(next_state)\n",
    "\n",
    "        return next_state, t\n",
    "\n",
    "    def update(self):\n",
    "        self.learning_steps += 1\n",
    "        states, actions, rewards, dones, next_states = self.buffer.sample(self.batch_size)\n",
    "\n",
    "        self.update_critic(states, actions, rewards, dones, next_states)\n",
    "        self.update_actor(states)\n",
    "        self.update_target()\n",
    "\n",
    "    def update_critic(self, states, actions, rewards, dones, next_states):\n",
    "        # 現在のソフト状態行動価値を計算する．\n",
    "        curr_qs1, curr_qs2 = self.critic(states, actions)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # ソフト状態価値のターゲットを計算します．\n",
    "            # target_vs = ...\n",
    "            next_actions, log_pis = self.actor.sample(next_states)\n",
    "            qs1, qs2 = self.critic_target(next_states, next_actions)\n",
    "            target_vs = torch.min(qs1, qs2) - self.alpha * log_pis\n",
    "\n",
    "        # ソフト状態行動価値のターゲットを計算します．\n",
    "        # target_qs = ...\n",
    "        target_qs = rewards * self.reward_scale + (1 - dones) * self.gamma * target_vs\n",
    "\n",
    "        loss_critic1 = (curr_qs1 - target_qs).pow_(2).mean()\n",
    "        loss_critic2 = (curr_qs2 - target_qs).pow_(2).mean()\n",
    "\n",
    "        self.optim_critic.zero_grad()\n",
    "        (loss_critic1 + loss_critic2).backward(retain_graph=False)\n",
    "        self.optim_critic.step()\n",
    "\n",
    "    def update_actor(self, states):\n",
    "        actions, log_pis = self.actor.sample(states)\n",
    "        qs1, qs2 = self.critic(states, actions)\n",
    "        loss_actor = (self.alpha * log_pis - torch.min(qs1, qs2)).mean()\n",
    "\n",
    "        self.optim_actor.zero_grad()\n",
    "        loss_actor.backward(retain_graph=False)\n",
    "        self.optim_actor.step()\n",
    "\n",
    "    def update_target(self):\n",
    "        for t, s in zip(self.critic_target.parameters(), self.critic.parameters()):\n",
    "            t.data.mul_(1.0 - self.tau)\n",
    "            t.data.add_(self.tau * s.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporated-meter",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_ID = 'CubeRoomSearchLikeContinuously-v0'\n",
    "target_machine_ip = 'localhost'\n",
    "SEED = 0\n",
    "REWARD_SCALE = 1.0\n",
    "NUM_STEPS = 5 * 10 ** 4\n",
    "EVAL_INTERVAL = 10 ** 3\n",
    "# EVAL_INTERVAL = 10\n",
    "\n",
    "map_enc = 100\n",
    "pose_enc = 50\n",
    "\n",
    "env = gym.make(ENV_ID, ip=target_machine_ip, gui=False, max_episode_steps=250)\n",
    "env_test = gym.make(ENV_ID, ip=target_machine_ip, gui=True, max_episode_steps=250)\n",
    "\n",
    "encoder = Encoder(map_enc, pose_enc, env.map_size)\n",
    "\n",
    "# s_shape = (env.observation_space['agent_pose'].low.size + env.observation_space['occupancy_grid'].low.size, )\n",
    "state_shape = (map_enc+pose_enc,)\n",
    "\n",
    "algo = SAC(\n",
    "    state_shape=state_shape,\n",
    "    action_shape=env.action_space.shape,\n",
    "    seed=SEED,\n",
    "    replay_size=10**6,\n",
    "    reward_scale=REWARD_SCALE,\n",
    "    encoder=encoder,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    env=env,\n",
    "    env_test=env_test,\n",
    "    algo=algo,\n",
    "    seed=SEED,\n",
    "    num_steps=NUM_STEPS,\n",
    "    eval_interval=EVAL_INTERVAL,\n",
    "    encoder=encoder,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coastal-ballet",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premier-confidentiality",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unnecessary-chaos",
   "metadata": {},
   "source": [
    "## Debug Zone"
   ]
  },
  {
   "cell_type": "raw",
   "id": "headed-blind",
   "metadata": {},
   "source": [
    "ENV_ID = 'CubeRoomSearchLikeContinuously-v0'\n",
    "target_machine_ip = 'localhost'\n",
    "map_enc = 100\n",
    "pose_enc = 50\n",
    "\n",
    "env = gym.make(ENV_ID, ip=target_machine_ip, gui=False, max_episode_steps=200)\n",
    "encoder = Encoder(map_enc, pose_enc, env.map_size)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "limiting-adapter",
   "metadata": {},
   "source": [
    "with torch.no_grad():\n",
    "    sta = encoder(env.observation_space.sample())\n",
    "actor = SACActor(action_shape=(3,), state_shape=(150,))\n",
    "with torch.no_grad():\n",
    "    print(actor(torch.tensor(sta, dtype=torch.float, device=torch.device('cpu')).unsqueeze(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "typical-crossing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funky-quarterly",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
