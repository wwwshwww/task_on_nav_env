{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "palestinian-theorem",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要なライブラリのインポート．\n",
    "from abc import ABC, abstractmethod\n",
    "import os\n",
    "import glob\n",
    "from collections import deque\n",
    "from time import time\n",
    "from datetime import timedelta\n",
    "import pickle\n",
    "from base64 import b64encode\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions import Normal\n",
    "import torch.nn.functional as F\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "\n",
    "import robo_gym\n",
    "from robo_gym.wrappers.exception_handling import ExceptionHandling\n",
    "\n",
    "# Gymの警告を一部無視する．\n",
    "gym.logger.set_level(40)\n",
    "# matplotlibをColab上で描画するためのコマンド．\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "chubby-character",
   "metadata": {},
   "outputs": [],
   "source": [
    "def atanh(x):\n",
    "    \"\"\" tanh の逆関数． \"\"\"\n",
    "    return 0.5 * (torch.log(1 + x + 1e-6) - torch.log(1 - x + 1e-6))\n",
    "\n",
    "\n",
    "def evaluate_lop_pi(means, log_stds, actions):\n",
    "    \"\"\" 平均(mean)，標準偏差の対数(log_stds)でパラメータ化した方策における，行動(actions)の確率密度の対数を計算する． \"\"\"\n",
    "    noises = (atanh(actions) - means) / (log_stds.exp() + 1e-8)\n",
    "    return calculate_log_pi(log_stds, noises, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "played-costs",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_log_pi(log_stds, noises, actions):\n",
    "    \"\"\" 確率論的な行動の確率密度を返す． \"\"\"\n",
    "\n",
    "    # NOTE: 入力はすべて (batch_size, |A|) となっているので，この関数では　batch_size　分の確率密度の対数 \\log \\pi(a|s) を\n",
    "    # それぞれ独立に計算し (batch_size, 1) で返します．\n",
    "\n",
    "    # ガウス分布 `N(0, stds * I)` における `noises * stds` の確率密度の対数(= \\log \\pi(u|a))を計算する．\n",
    "    stds = log_stds.exp()\n",
    "    gaussian_log_probs = Normal(torch.zeros_like(stds), stds).log_prob(stds * noises).sum(dim=-1, keepdim=True)\n",
    "\n",
    "    # NOTE: gaussian_log_probs には (batch_size, 1) で表された確率密度の対数 \\log p(u|s) が入っています．\n",
    "\n",
    "    log_pis = gaussian_log_probs - torch.log(1 - actions**2 + 1e-6).sum(dim=-1, keepdim=True)\n",
    "\n",
    "    return log_pis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "intelligent-seattle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reparameterize(means, log_stds):\n",
    "    \"\"\" Reparameterization Trickを用いて，確率論的な行動とその確率密度を返す． \"\"\"\n",
    "\n",
    "    # 標準偏差．\n",
    "    stds = log_stds.exp()\n",
    "\n",
    "    noises = torch.randn_like(means)\n",
    "    actions = torch.tanh(means+noises*stds)\n",
    "\n",
    "    # 確率論的な行動の確率密度の対数を計算する．\n",
    "    log_pis = calculate_log_pi(log_stds, noises, actions)\n",
    "\n",
    "    return actions, log_pis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "australian-partition",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_state_dim(state):\n",
    "    return np.concatenate([state['agent_pose'], state['occupancy_grid']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "attended-archive",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SACActor(nn.Module):\n",
    "\n",
    "    def __init__(self, state_shape, action_shape):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_shape[0], 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 2 * action_shape[0]),\n",
    "        )\n",
    "        \n",
    "        torch.nn.init.xavier_uniform_(self.net[0].weight)\n",
    "        torch.nn.init.xavier_uniform_(self.net[2].weight)\n",
    "        torch.nn.init.xavier_uniform_(self.net[4].weight, gain=1.0)\n",
    "\n",
    "    def forward(self, states):\n",
    "        means, log_stds = self.net(states).chunk(2, dim=-1)\n",
    "        return torch.tanh(means)\n",
    "\n",
    "    def sample(self, states):\n",
    "        means, log_stds = self.net(states).chunk(2, dim=-1)\n",
    "        log_stds = log_stds.clamp(-20, 2)\n",
    "        return reparameterize(means, log_stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "imported-overview",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SACCritic(nn.Module):\n",
    "\n",
    "    def __init__(self, state_shape, action_shape):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net1 = nn.Sequential(\n",
    "            nn.Linear(state_shape[0] + action_shape[0], 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "        self.net2 = nn.Sequential(\n",
    "            nn.Linear(state_shape[0] + action_shape[0], 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "        \n",
    "        for i in [0,2,4]:\n",
    "            torch.nn.init.xavier_uniform_(self.net1[i].weight)\n",
    "            torch.nn.init.xavier_uniform_(self.net2[i].weight)\n",
    "\n",
    "    def forward(self, states, actions):\n",
    "        sa = torch.cat([states, actions], dim=-1)\n",
    "        q1 = self.net1(sa)\n",
    "        q2 = self.net2(sa)\n",
    "        return q1, q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "unlikely-infrastructure",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "\n",
    "    def __init__(self, buffer_size, state_shape, action_shape, device):\n",
    "        # 次にデータを挿入するインデックス．\n",
    "        self._p = 0\n",
    "        # データ数．\n",
    "        self._n = 0\n",
    "        # リプレイバッファのサイズ．\n",
    "        self.buffer_size = buffer_size\n",
    "\n",
    "        # GPU上に保存するデータ．\n",
    "        self.states = torch.empty((buffer_size, *state_shape), dtype=torch.float, device=device)\n",
    "        self.actions = torch.empty((buffer_size, *action_shape), dtype=torch.float, device=device)\n",
    "        self.rewards = torch.empty((buffer_size, 1), dtype=torch.float, device=device)\n",
    "        self.dones = torch.empty((buffer_size, 1), dtype=torch.float, device=device)\n",
    "        self.next_states = torch.empty((buffer_size, *state_shape), dtype=torch.float, device=device)\n",
    "\n",
    "    def append(self, state, action, reward, done, next_state):\n",
    "        self.states[self._p].copy_(torch.from_numpy(state))\n",
    "        self.actions[self._p].copy_(torch.from_numpy(action))\n",
    "        self.rewards[self._p] = float(reward)\n",
    "        self.dones[self._p] = float(done)\n",
    "        self.next_states[self._p].copy_(torch.from_numpy(next_state))\n",
    "\n",
    "        self._p = (self._p + 1) % self.buffer_size\n",
    "        self._n = min(self._n + 1, self.buffer_size)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        idxes = np.random.randint(low=0, high=self._n, size=batch_size)\n",
    "        return (\n",
    "            self.states[idxes],\n",
    "            self.actions[idxes],\n",
    "            self.rewards[idxes],\n",
    "            self.dones[idxes],\n",
    "            self.next_states[idxes]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "velvet-fields",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Algorithm(ABC):\n",
    "\n",
    "    def explore(self, state):\n",
    "        \"\"\" 確率論的な行動と，その行動の確率密度の対数 \\log(\\pi(a|s)) を返す． \"\"\"\n",
    "        state = torch.tensor(state, dtype=torch.float, device=self.device).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            action, log_pi = self.actor.sample(state)\n",
    "        return action.cpu().numpy()[0], log_pi.item()\n",
    "\n",
    "    def exploit(self, state):\n",
    "        \"\"\" 決定論的な行動を返す． \"\"\"\n",
    "        state = torch.tensor(state, dtype=torch.float, device=self.device).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            action = self.actor(state)\n",
    "        return action.cpu().numpy()[0]\n",
    "\n",
    "    @abstractmethod\n",
    "    def is_update(self, steps):\n",
    "        \"\"\" 現在のトータルのステップ数(steps)を受け取り，アルゴリズムを学習するか否かを返す． \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def step(self, env, state, t, steps):\n",
    "        \"\"\" 環境(env)，現在の状態(state)，現在のエピソードのステップ数(t)，今までのトータルのステップ数(steps)を\n",
    "            受け取り，リプレイバッファへの保存などの処理を行い，状態・エピソードのステップ数を更新する．\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def update(self):\n",
    "        \"\"\" 1回分の学習を行う． \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "amber-prayer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init(module):\n",
    "    nn.init.orthogonal_(module.weight.data, gain=nn.init.calculate_gain('relu'))\n",
    "    nn.init.constant_(module.bias.data, 0)\n",
    "    return module\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, map_output=100, pose_output=50, map_size=128):\n",
    "        super().__init__()\n",
    "        self.map_size = map_size\n",
    "\n",
    "        self.map_conv = nn.Sequential(\n",
    "            # 128*128 -> 32*32\n",
    "            init(nn.Conv2d(1, 16, kernel_size=8, stride=4, padding=2)),\n",
    "            nn.ReLU(),\n",
    "            # 32*32 -> 15*15\n",
    "            init(nn.Conv2d(16, 32, kernel_size=4, stride=2)),\n",
    "            nn.ReLU(),\n",
    "            # 15*15 -> 13*13\n",
    "            init(nn.Conv2d(32, 32, kernel_size=3, stride=1)),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            # 32*13*13 -> map_output\n",
    "            init(nn.Linear(32*13*13, map_output)),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.pose_buf = nn.Sequential(\n",
    "            # 3 -> pose_output\n",
    "            init(nn.Linear(3, pose_output)),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, obs):\n",
    "        map_img = obs['occupancy_grid'].reshape((self.map_size, self.map_size)).T\n",
    "        map_img = torch.tensor(map_img, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "        pose = torch.tensor(obs['agent_pose'], dtype=torch.float32)\n",
    "        \n",
    "        map_out = self.map_conv(map_img).squeeze()\n",
    "        pose_out = self.pose_buf(pose).squeeze()\n",
    "        return torch.cat([map_out, pose_out]).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "optical-delta",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "\n",
    "    def __init__(self, env, env_test, algo, encoder, seed=0, num_steps=10**6, eval_interval=10**4, num_eval_episodes=3):\n",
    "\n",
    "        self.env = env\n",
    "        self.env_test = env_test\n",
    "        self.algo = algo\n",
    "        self.encoder = encoder\n",
    "\n",
    "#         # 環境の乱数シードを設定する．\n",
    "#         self.env.seed(seed)\n",
    "#         self.env_test.seed(2**31-seed)\n",
    "\n",
    "        # 平均収益を保存するための辞書．\n",
    "        self.returns = {'step': [], 'return': []}\n",
    "\n",
    "        # データ収集を行うステップ数．\n",
    "        self.num_steps = num_steps\n",
    "        # 評価の間のステップ数(インターバル)．\n",
    "        self.eval_interval = eval_interval\n",
    "        # 評価を行うエピソード数．\n",
    "        self.num_eval_episodes = num_eval_episodes\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\" num_stepsステップの間，データ収集・学習・評価を繰り返す． \"\"\"\n",
    "\n",
    "        # 学習開始の時間\n",
    "        self.start_time = time()\n",
    "        # エピソードのステップ数．\n",
    "        t = 0\n",
    "\n",
    "        # 環境を初期化する．\n",
    "        # reset SLAM and generate new initial pose\n",
    "        state = self.env.reset(new_room=False, new_agent_pose=True)\n",
    "        with torch.no_grad():\n",
    "            state = self.encoder(state)\n",
    "\n",
    "        for steps in range(1, self.num_steps + 1):\n",
    "            # 環境(self.env)，現在の状態(state)，現在のエピソードのステップ数(t)，今までのトータルのステップ数(steps)を\n",
    "            # アルゴリズムに渡し，状態・エピソードのステップ数を更新する．\n",
    "            \n",
    "            state, t = self.algo.step(self.env, state, t, steps)\n",
    "\n",
    "            # アルゴリズムが準備できていれば，1回学習を行う．\n",
    "            if self.algo.is_update(steps):\n",
    "                self.algo.update()\n",
    "\n",
    "            # 一定のインターバルで評価する．\n",
    "            if steps % self.eval_interval == 0:\n",
    "                self.evaluate(steps)\n",
    "                state = self.env_test.reset(new_room=False, new_agent_pose=True)\n",
    "                with torch.no_grad():\n",
    "                    state = self.encoder(state)\n",
    "                \n",
    "    def evaluate(self, steps):\n",
    "        \"\"\" 複数エピソード環境を動かし，平均収益を記録する． \"\"\"\n",
    "\n",
    "        returns = []\n",
    "        state = self.env_test.reset(new_room=True, new_agent_pose=True)\n",
    "        with torch.no_grad():\n",
    "            state = self.encoder(state)\n",
    "        \n",
    "        for _ in range(self.num_eval_episodes):\n",
    "            state = self.env_test.reset(new_room=False, new_agent_pose=True)\n",
    "            with torch.no_grad():\n",
    "                state = self.encoder(state)\n",
    "            done = False\n",
    "            episode_return = 0.0\n",
    "\n",
    "            while (not done):\n",
    "                action = self.algo.exploit(state)\n",
    "                state, reward, done, _ = self.env_test.step(action)\n",
    "                with torch.no_grad():\n",
    "                    state = self.encoder(state)\n",
    "                episode_return += reward\n",
    "\n",
    "            returns.append(episode_return)\n",
    "\n",
    "        mean_return = np.mean(returns)\n",
    "        self.returns['step'].append(steps)\n",
    "        self.returns['return'].append(mean_return)\n",
    "\n",
    "        print(f'Num steps: {steps:<6}   '\n",
    "              f'Return: {mean_return:<5.1f}   '\n",
    "              f'Time: {self.time}')\n",
    "        \n",
    "    def plot(self):\n",
    "        \"\"\" 平均収益のグラフを描画する． \"\"\"\n",
    "        fig = plt.figure(figsize=(8, 6))\n",
    "        plt.plot(self.returns['step'], self.returns['return'])\n",
    "        plt.xlabel('Steps', fontsize=24)\n",
    "        plt.ylabel('Return', fontsize=24)\n",
    "        plt.tick_params(labelsize=18)\n",
    "        plt.title(f'{self.env.unwrapped.spec.id}', fontsize=24)\n",
    "        plt.tight_layout()\n",
    "\n",
    "    @property\n",
    "    def time(self):\n",
    "        \"\"\" 学習開始からの経過時間． \"\"\"\n",
    "        return str(timedelta(seconds=int(time() - self.start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "third-jamaica",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAC(Algorithm):\n",
    "\n",
    "    def __init__(self, state_shape, action_shape, encoder, device=torch.device('cuda'), seed=0,\n",
    "                 batch_size=256, gamma=0.99, lr_actor=3e-4, lr_critic=3e-4,\n",
    "                 replay_size=10**6, start_steps=10**4, tau=5e-3, alpha=0.2, reward_scale=1.0):\n",
    "        super().__init__()\n",
    "\n",
    "        # シードを設定する．\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "        # リプレイバッファ．\n",
    "        self.buffer = ReplayBuffer(\n",
    "            buffer_size=replay_size,\n",
    "            state_shape=state_shape,\n",
    "            action_shape=action_shape,\n",
    "            device=device,\n",
    "        )\n",
    "        \n",
    "        self.encoder = encoder\n",
    "\n",
    "        # Actor-Criticのネットワークを構築する．\n",
    "        self.actor = SACActor(\n",
    "            state_shape=state_shape,\n",
    "            action_shape=action_shape\n",
    "        ).to(device)\n",
    "        self.critic = SACCritic(\n",
    "            state_shape=state_shape,\n",
    "            action_shape=action_shape\n",
    "        ).to(device)\n",
    "        self.critic_target = SACCritic(\n",
    "            state_shape=state_shape,\n",
    "            action_shape=action_shape\n",
    "        ).to(device).eval()\n",
    "\n",
    "        # ターゲットネットワークの重みを初期化し，勾配計算を無効にする．\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "        for param in self.critic_target.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # オプティマイザ．\n",
    "        self.optim_actor = torch.optim.Adam(self.actor.parameters(), lr=lr_actor)\n",
    "        self.optim_critic = torch.optim.Adam(self.critic.parameters(), lr=lr_critic)\n",
    "\n",
    "        # その他パラメータ．\n",
    "        self.learning_steps = 0\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        self.gamma = gamma\n",
    "        self.start_steps = start_steps\n",
    "        self.tau = tau\n",
    "        self.alpha = alpha\n",
    "        self.reward_scale = reward_scale\n",
    "\n",
    "    def is_update(self, steps):\n",
    "        # 学習初期の一定期間(start_steps)は学習しない．\n",
    "        return steps >= max(self.start_steps, self.batch_size)\n",
    "\n",
    "    def step(self, env, state, t, steps):\n",
    "        t += 1\n",
    "\n",
    "        # 学習初期の一定期間(start_steps)は，ランダムに行動して多様なデータの収集を促進する．\n",
    "        if steps <= self.start_steps:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action, _ = self.explore(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        with torch.no_grad():\n",
    "            next_state = self.encoder(next_state)\n",
    "        \n",
    "        \n",
    "        # not ignore done\n",
    "\n",
    "        # リプレイバッファにデータを追加する．\n",
    "        self.buffer.append(state, action, reward, done, next_state)\n",
    "\n",
    "        # エピソードが終了した場合には，環境をリセットする．\n",
    "        if done:\n",
    "            t = 0\n",
    "            next_state = env.reset()\n",
    "            with torch.no_grad():\n",
    "                next_state = self.encoder(next_state)\n",
    "\n",
    "        return next_state, t\n",
    "\n",
    "    def update(self):\n",
    "        self.learning_steps += 1\n",
    "        states, actions, rewards, dones, next_states = self.buffer.sample(self.batch_size)\n",
    "\n",
    "        self.update_critic(states, actions, rewards, dones, next_states)\n",
    "        self.update_actor(states)\n",
    "        self.update_target()\n",
    "\n",
    "    def update_critic(self, states, actions, rewards, dones, next_states):\n",
    "        # 現在のソフト状態行動価値を計算する．\n",
    "        curr_qs1, curr_qs2 = self.critic(states, actions)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # ソフト状態価値のターゲットを計算します．\n",
    "            # target_vs = ...\n",
    "            next_actions, log_pis = self.actor.sample(next_states)\n",
    "            qs1, qs2 = self.critic_target(next_states, next_actions)\n",
    "            target_vs = torch.min(qs1, qs2) - self.alpha * log_pis\n",
    "\n",
    "        # ソフト状態行動価値のターゲットを計算します．\n",
    "        # target_qs = ...\n",
    "        target_qs = rewards * self.reward_scale + (1 - dones) * self.gamma * target_vs\n",
    "\n",
    "        loss_critic1 = (curr_qs1 - target_qs).pow_(2).mean()\n",
    "        loss_critic2 = (curr_qs2 - target_qs).pow_(2).mean()\n",
    "\n",
    "        self.optim_critic.zero_grad()\n",
    "        (loss_critic1 + loss_critic2).backward(retain_graph=False)\n",
    "        self.optim_critic.step()\n",
    "\n",
    "    def update_actor(self, states):\n",
    "        actions, log_pis = self.actor.sample(states)\n",
    "        qs1, qs2 = self.critic(states, actions)\n",
    "        loss_actor = (self.alpha * log_pis - torch.min(qs1, qs2)).mean()\n",
    "\n",
    "        self.optim_actor.zero_grad()\n",
    "        loss_actor.backward(retain_graph=False)\n",
    "        self.optim_actor.step()\n",
    "\n",
    "    def update_target(self):\n",
    "        for t, s in zip(self.critic_target.parameters(), self.critic.parameters()):\n",
    "            t.data.mul_(1.0 - self.tau)\n",
    "            t.data.add_(self.tau * s.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "incorporated-meter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting new Robot Server | Tentative 1\n",
      "<class 'server_manager_pb2.RobotServer'>\n",
      "True \n",
      "Successfully started Robot Server at localhost:39781\n"
     ]
    }
   ],
   "source": [
    "ENV_ID = 'CubeRoomSearchLikeContinuously-v0'\n",
    "SEED = 0\n",
    "REWARD_SCALE = 1.0\n",
    "NUM_STEPS = 5 * 10 ** 4\n",
    "EVAL_INTERVAL = 10 ** 3\n",
    "# EVAL_INTERVAL = 100\n",
    "TIME_LIMIT = 50\n",
    "\n",
    "map_enc = 100\n",
    "pose_enc = 50\n",
    "target_machine_ip = 'localhost'\n",
    "\n",
    "env = gym.make(ENV_ID, ip=target_machine_ip, gui=False, max_episode_steps=TIME_LIMIT)\n",
    "env_test = gym.make(ENV_ID, ip=target_machine_ip, gui=False, max_episode_steps=TIME_LIMIT)\n",
    "\n",
    "encoder = Encoder(map_enc, pose_enc, env.map_size)\n",
    "\n",
    "# s_shape = (env.observation_space['agent_pose'].low.size + env.observation_space['occupancy_grid'].low.size, )\n",
    "state_shape = (map_enc+pose_enc,)\n",
    "\n",
    "algo = SAC(\n",
    "    state_shape=state_shape,\n",
    "    action_shape=env.action_space.shape,\n",
    "    seed=SEED,\n",
    "    replay_size=10**6,\n",
    "    start_steps=EVAL_INTERVAL,\n",
    "    reward_scale=REWARD_SCALE,\n",
    "    encoder=encoder,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    env=env,\n",
    "    env_test=env_test,\n",
    "    algo=algo,\n",
    "    seed=SEED,\n",
    "    num_steps=NUM_STEPS,\n",
    "    eval_interval=EVAL_INTERVAL,\n",
    "    encoder=encoder,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "coastal-ballet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.30378205  0.6961988   0.9572102 ]\n",
      "[-0.3021372  0.7072784  0.9516622]\n",
      "[-0.30173823  0.7068466   0.95183694]\n",
      "[-0.30049965  0.70533717  0.95252585]\n",
      "[-0.29995206  0.7044391   0.9529857 ]\n",
      "[-0.29958034  0.7037933   0.9533205 ]\n",
      "[-0.2995038  0.7036594  0.9533896]\n",
      "[-0.2994185   0.70351005  0.9534664 ]\n",
      "[-0.29937834  0.7033349   0.95357037]\n",
      "[-0.29933104  0.7031148   0.953702  ]\n",
      "[-0.29931116  0.70302695  0.95375437]\n",
      "[-0.29929093  0.7029244   0.9538152 ]\n",
      "[-0.29925847  0.7027625   0.9539114 ]\n",
      "[-0.29922837  0.70261246  0.95400023]\n",
      "[-0.29922336  0.70251924  0.9540555 ]\n",
      "[-0.29932186  0.7024297   0.95410967]\n",
      "[-0.29943365  0.7023289   0.95417047]\n",
      "[-0.2995214   0.70224917  0.95421857]\n",
      "[-0.29960334  0.7021748   0.95426345]\n",
      "[-0.2996657   0.7021183   0.95429856]\n",
      "[0.32676965 0.83642286 0.977546  ]\n",
      "[0.33759943 0.8339651  0.9766209 ]\n",
      "[0.33804128 0.83343023 0.97650427]\n",
      "[0.33865777 0.8330014  0.97640264]\n",
      "[0.33985543 0.83243024 0.9763091 ]\n",
      "[0.34152135 0.8316095  0.97620356]\n",
      "[0.3432427  0.83075464 0.97609276]\n",
      "[0.34226614 0.83124644 0.97615945]\n",
      "[0.34381646 0.830474   0.9760591 ]\n",
      "[0.34581038 0.8294732  0.97592837]\n",
      "[0.34745947 0.82883424 0.9758191 ]\n",
      "[0.35019344 0.82835275 0.9756562 ]\n",
      "[0.35252684 0.828091   0.9755253 ]\n",
      "[0.35413677 0.82793844 0.9754304 ]\n",
      "[0.3548986 0.827827  0.9753977]\n",
      "[0.354814   0.8275755  0.97534853]\n",
      "[0.35473055 0.8272744  0.9752889 ]\n",
      "[0.35469636 0.8269073  0.9752181 ]\n",
      "[0.35466066 0.82658774 0.9751549 ]\n",
      "[0.35458866 0.8263077  0.97508955]\n",
      "[0.9660404  0.38794586 0.99159384]\n",
      "[0.9986053 0.2871375 0.9999173]\n",
      "[0.9994368  0.7877795  0.99998116]\n",
      "[0.9994368  0.7877734  0.99998116]\n",
      "[0.99943686 0.7877712  0.99998116]\n",
      "[0.99943686 0.7877703  0.99998116]\n",
      "[0.99943686 0.7877706  0.99998116]\n",
      "[0.9994368  0.7877711  0.99998116]\n",
      "[0.99943686 0.7877716  0.99998116]\n",
      "[0.9994368  0.7877722  0.99998116]\n",
      "[0.9994368  0.7877727  0.99998116]\n",
      "[0.9994368  0.7877737  0.99998116]\n",
      "[0.9994368  0.7877747  0.99998116]\n",
      "[0.9994368  0.7877756  0.99998116]\n",
      "[0.9994368  0.7877766  0.99998116]\n",
      "[0.9994368  0.78777754 0.99998116]\n",
      "[0.9994368  0.78777885 0.99998116]\n",
      "[0.9994368  0.7877804  0.99998116]\n",
      "[0.9994368  0.7877822  0.99998116]\n",
      "[0.9994368  0.7877841  0.99998116]\n",
      "Num steps: 1000     Return: 15.7    Time: 0:50:39\n",
      "[-0.27000886  0.5174004   0.973497  ]\n",
      "[-0.312466    0.47643587  0.9739149 ]\n",
      "[-0.53819394  0.8450657   0.99541754]\n",
      "[-0.5381956   0.8450638   0.99541754]\n",
      "[-0.538199    0.8450643   0.99541754]\n",
      "[-0.53820205  0.8450644   0.9954176 ]\n",
      "[-0.5382065   0.84506416  0.9954176 ]\n",
      "[-0.5382097   0.84506387  0.9954176 ]\n",
      "[-0.5382159   0.8450657   0.99541765]\n",
      "[-0.5382167   0.8450657   0.99541765]\n",
      "[-0.538217    0.84506553  0.99541765]\n",
      "[-0.5382172   0.8450653   0.99541765]\n",
      "[-0.5382172   0.8450653   0.99541765]\n",
      "[-0.5382169   0.8450652   0.99541765]\n",
      "[-0.53821707  0.8450651   0.99541765]\n",
      "[-0.538217    0.845065    0.99541765]\n",
      "[-0.5382165   0.8450649   0.99541765]\n",
      "[-0.53821653  0.8450646   0.99541765]\n",
      "[-0.53821635  0.8450645   0.99541765]\n",
      "[-0.5382165   0.8450645   0.99541765]\n",
      "[0.26913875 0.85840696 0.8352907 ]\n",
      "[0.2080324 0.8961906 0.8965584]\n",
      "[0.88163716 0.9988523  0.994425  ]\n",
      "[0.9806117  0.9989253  0.99634874]\n",
      "[0.98061097 0.9989253  0.99634886]\n",
      "[0.98061275 0.9989254  0.9963489 ]\n",
      "[0.9806117 0.9989254 0.996349 ]\n",
      "[0.98061246 0.9989254  0.99634904]\n",
      "[0.98737764 0.9990792  0.99552715]\n",
      "[0.9837997  0.9994176  0.99196815]\n",
      "[0.98256624 0.99923146 0.99347955]\n",
      "[0.99328434 0.9989693  0.9940837 ]\n",
      "[0.99328434 0.99896926 0.99408364]\n",
      "[0.99328434 0.99896926 0.9940837 ]\n",
      "[0.9932843  0.99896926 0.9940837 ]\n",
      "[0.9932843  0.99896926 0.9940837 ]\n",
      "[0.99328434 0.99896926 0.99408364]\n",
      "[0.99328434 0.9989693  0.99408364]\n",
      "[0.9932844 0.9989693 0.9940836]\n",
      "[0.99328446 0.9989694  0.99408346]\n",
      "[0.5792275  0.8352395  0.91672206]\n",
      "[0.5693841  0.84977007 0.9322009 ]\n",
      "[0.569312   0.8498479  0.93223965]\n",
      "[0.5692156  0.84995306 0.9322926 ]\n",
      "[0.569124  0.8500542 0.9323433]\n",
      "[0.5690379  0.8501767  0.93239474]\n",
      "[0.568949  0.8503252 0.9324514]\n",
      "[0.5688526  0.85048586 0.93251276]\n",
      "[0.5687585  0.8506493  0.93257564]\n",
      "[0.56869006 0.85076976 0.93262213]\n",
      "[0.5685855  0.8509543  0.93269324]\n",
      "[0.5684943  0.85112184 0.93275833]\n",
      "[0.5684252  0.85125333 0.93280965]\n",
      "[0.568348   0.8514059  0.93286955]\n",
      "[0.568283   0.85153854 0.9329218 ]\n",
      "[0.5682366 0.8516551 0.9329647]\n",
      "[0.56821424 0.8517485  0.9329933 ]\n",
      "[0.56820166 0.8517761  0.9330016 ]\n",
      "[0.5682001  0.8517786  0.93300223]\n",
      "[0.56820124 0.8517764  0.9330016 ]\n",
      "Num steps: 2000     Return: -1.0    Time: 1:40:18\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-3435b262f1ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-fae25891ce98>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;31m# アルゴリズムに渡し，状態・エピソードのステップ数を更新する．\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;31m# アルゴリズムが準備できていれば，1回学習を行う．\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-58947efa692b>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, env, state, t, steps)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/robo_gym/envs/mir_nav/mir_nav.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;31m# Send action to Robot Server\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrs_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRobotServerError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"send_action\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/robo_gym_server_modules/robot_server/client.py\u001b[0m in \u001b[0;36msend_action\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msend_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrobot_server_stub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSendAction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrobot_server_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuccess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/grpc/_channel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m    920\u001b[0m                  compression=None):\n\u001b[1;32m    921\u001b[0m         state, call, = self._blocking(request, timeout, metadata, credentials,\n\u001b[0;32m--> 922\u001b[0;31m                                       wait_for_ready, compression)\n\u001b[0m\u001b[1;32m    923\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_end_unary_response_blocking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/grpc/_channel.py\u001b[0m in \u001b[0;36m_blocking\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m    908\u001b[0m                     \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m                 ),), self._context)\n\u001b[0;32m--> 910\u001b[0;31m             \u001b[0mevent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m             \u001b[0m_handle_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_response_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi\u001b[0m in \u001b[0;36mgrpc._cython.cygrpc.SegregatedCall.next_event\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi\u001b[0m in \u001b[0;36mgrpc._cython.cygrpc._next_call_event\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi\u001b[0m in \u001b[0;36mgrpc._cython.cygrpc._next_call_event\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi\u001b[0m in \u001b[0;36mgrpc._cython.cygrpc._latent_event\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi\u001b[0m in \u001b[0;36mgrpc._cython.cygrpc._next\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premier-confidentiality",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sensitive-hindu",
   "metadata": {},
   "source": [
    "## Debug Zone"
   ]
  },
  {
   "cell_type": "raw",
   "id": "timely-secretary",
   "metadata": {},
   "source": [
    "ENV_ID = 'CubeRoomSearchLikeContinuously-v0'\n",
    "target_machine_ip = 'localhost'\n",
    "map_enc = 100\n",
    "pose_enc = 50\n",
    "\n",
    "env = gym.make(ENV_ID, ip=target_machine_ip, gui=False, max_episode_steps=200)\n",
    "encoder = Encoder(map_enc, pose_enc, env.map_size)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "specified-arrival",
   "metadata": {},
   "source": [
    "with torch.no_grad():\n",
    "    sta = encoder(env.observation_space.sample())\n",
    "actor = SACActor(action_shape=(3,), state_shape=(150,))\n",
    "with torch.no_grad():\n",
    "    print(actor(torch.tensor(sta, dtype=torch.float, device=torch.device('cpu')).unsqueeze(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "herbal-slope",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adult-harvest",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
