{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cleared-intervention",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要なライブラリのインポート．\n",
    "from abc import ABC, abstractmethod\n",
    "import os\n",
    "import glob\n",
    "from collections import deque\n",
    "from time import time\n",
    "from datetime import timedelta\n",
    "import pickle\n",
    "from base64 import b64encode\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions import Normal\n",
    "import torch.nn.functional as F\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "\n",
    "import robo_gym\n",
    "from robo_gym.wrappers.exception_handling import ExceptionHandling\n",
    "\n",
    "# Gymの警告を一部無視する．\n",
    "gym.logger.set_level(40)\n",
    "# matplotlibをColab上で描画するためのコマンド．\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "julian-portugal",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_state_dim(state):\n",
    "    return np.concatenate([state['agent_pose'], state['occupancy_grid']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "white-bacon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def atanh(x):\n",
    "    \"\"\" tanh の逆関数． \"\"\"\n",
    "    return 0.5 * (torch.log(1 + x + 1e-6) - torch.log(1 - x + 1e-6))\n",
    "\n",
    "\n",
    "def evaluate_lop_pi(means, log_stds, actions):\n",
    "    \"\"\" 平均(mean)，標準偏差の対数(log_stds)でパラメータ化した方策における，行動(actions)の確率密度の対数を計算する． \"\"\"\n",
    "    noises = (atanh(actions) - means) / (log_stds.exp() + 1e-8)\n",
    "    return calculate_log_pi(log_stds, noises, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "affecting-broad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_log_pi(log_stds, noises, actions):\n",
    "    \"\"\" 確率論的な行動の確率密度を返す． \"\"\"\n",
    "\n",
    "    # NOTE: 入力はすべて (batch_size, |A|) となっているので，この関数では　batch_size　分の確率密度の対数 \\log \\pi(a|s) を\n",
    "    # それぞれ独立に計算し (batch_size, 1) で返します．\n",
    "\n",
    "    # ガウス分布 `N(0, stds * I)` における `noises * stds` の確率密度の対数(= \\log \\pi(u|a))を計算する．\n",
    "    stds = log_stds.exp()\n",
    "    gaussian_log_probs = Normal(torch.zeros_like(stds), stds).log_prob(stds * noises).sum(dim=-1, keepdim=True)\n",
    "\n",
    "    # NOTE: gaussian_log_probs には (batch_size, 1) で表された確率密度の対数 \\log p(u|s) が入っています．\n",
    "\n",
    "    # [演習] その後，tanh による確率密度の変化を修正しましょう．\n",
    "    # (例)\n",
    "    # log_pis = gaussian_log_probs - ...\n",
    "    log_pis = gaussian_log_probs - torch.log(1 - actions**2 + 1e-6).sum(dim=-1, keepdim=True)\n",
    "\n",
    "    return log_pis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "unusual-export",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reparameterize(means, log_stds):\n",
    "    \"\"\" Reparameterization Trickを用いて，確率論的な行動とその確率密度を返す． \"\"\"\n",
    "\n",
    "    # 標準偏差．\n",
    "    stds = log_stds.exp()\n",
    "\n",
    "    # [演習] Reparameterization Trickを用いて，標準ガウス分布からノイズをサンプリングし，確率論的な行動を計算しましょう．\n",
    "    # (例)\n",
    "    # noises = ...\n",
    "    # actions = ...\n",
    "    noises = torch.randn_like(means)\n",
    "    actions = torch.tanh(means+noises*stds)\n",
    "\n",
    "    # 確率論的な行動の確率密度の対数を計算する．\n",
    "    log_pis = calculate_log_pi(log_stds, noises, actions)\n",
    "\n",
    "    return actions, log_pis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "native-begin",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "\n",
    "    def __init__(self, env, env_test, algo, seed=0, num_steps=10**6, eval_interval=10**4, num_eval_episodes=3):\n",
    "\n",
    "        self.env = env\n",
    "        self.env_test = env_test\n",
    "        self.algo = algo\n",
    "\n",
    "#         # 環境の乱数シードを設定する．\n",
    "#         self.env.seed(seed)\n",
    "#         self.env_test.seed(2**31-seed)\n",
    "\n",
    "        # 平均収益を保存するための辞書．\n",
    "        self.returns = {'step': [], 'return': []}\n",
    "\n",
    "        # データ収集を行うステップ数．\n",
    "        self.num_steps = num_steps\n",
    "        # 評価の間のステップ数(インターバル)．\n",
    "        self.eval_interval = eval_interval\n",
    "        # 評価を行うエピソード数．\n",
    "        self.num_eval_episodes = num_eval_episodes\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\" num_stepsステップの間，データ収集・学習・評価を繰り返す． \"\"\"\n",
    "\n",
    "        # 学習開始の時間\n",
    "        self.start_time = time()\n",
    "        # エピソードのステップ数．\n",
    "        t = 0\n",
    "\n",
    "        # 環境を初期化する．\n",
    "        # reset SLAM and generate new initial pose\n",
    "        state = self.env.reset(new_room=False, new_agent_pose=True)\n",
    "        state = fix_state_dim(state)\n",
    "\n",
    "        for steps in range(1, self.num_steps + 1):\n",
    "            # 環境(self.env)，現在の状態(state)，現在のエピソードのステップ数(t)，今までのトータルのステップ数(steps)を\n",
    "            # アルゴリズムに渡し，状態・エピソードのステップ数を更新する．\n",
    "            state, t = self.algo.step(self.env, state, t, steps)\n",
    "\n",
    "            # アルゴリズムが準備できていれば，1回学習を行う．\n",
    "            if self.algo.is_update(steps):\n",
    "                self.algo.update()\n",
    "\n",
    "            # 一定のインターバルで評価する．\n",
    "            if steps % self.eval_interval == 0:\n",
    "                self.evaluate(steps)\n",
    "                \n",
    "    def evaluate(self, steps):\n",
    "        \"\"\" 複数エピソード環境を動かし，平均収益を記録する． \"\"\"\n",
    "\n",
    "        returns = []\n",
    "        for _ in range(self.num_eval_episodes):\n",
    "            state = self.env_test.reset()\n",
    "            state = fix_state_dim(state)\n",
    "            done = False\n",
    "            episode_return = 0.0\n",
    "\n",
    "            while (not done):\n",
    "                action = self.algo.exploit(state)\n",
    "                print(action)\n",
    "                state, reward, done, _ = self.env_test.step(action)\n",
    "                state = fix_state_dim(state)\n",
    "                episode_return += reward\n",
    "\n",
    "            returns.append(episode_return)\n",
    "\n",
    "        mean_return = np.mean(returns)\n",
    "        self.returns['step'].append(steps)\n",
    "        self.returns['return'].append(mean_return)\n",
    "\n",
    "        print(f'Num steps: {steps:<6}   '\n",
    "              f'Return: {mean_return:<5.1f}   '\n",
    "              f'Time: {self.time}')\n",
    "        \n",
    "    def plot(self):\n",
    "        \"\"\" 平均収益のグラフを描画する． \"\"\"\n",
    "        fig = plt.figure(figsize=(8, 6))\n",
    "        plt.plot(self.returns['step'], self.returns['return'])\n",
    "        plt.xlabel('Steps', fontsize=24)\n",
    "        plt.ylabel('Return', fontsize=24)\n",
    "        plt.tick_params(labelsize=18)\n",
    "        plt.title(f'{self.env.unwrapped.spec.id}', fontsize=24)\n",
    "        plt.tight_layout()\n",
    "\n",
    "    @property\n",
    "    def time(self):\n",
    "        \"\"\" 学習開始からの経過時間． \"\"\"\n",
    "        return str(timedelta(seconds=int(time() - self.start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "administrative-drawing",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Algorithm(ABC):\n",
    "\n",
    "    def explore(self, state):\n",
    "        \"\"\" 確率論的な行動と，その行動の確率密度の対数 \\log(\\pi(a|s)) を返す． \"\"\"\n",
    "        state = torch.tensor(state, dtype=torch.float, device=self.device).unsqueeze_(0)\n",
    "        with torch.no_grad():\n",
    "            action, log_pi = self.actor.sample(state)\n",
    "        return action.cpu().numpy()[0], log_pi.item()\n",
    "\n",
    "    def exploit(self, state):\n",
    "        \"\"\" 決定論的な行動を返す． \"\"\"\n",
    "        state = torch.tensor(state, dtype=torch.float, device=self.device).unsqueeze_(0)\n",
    "        with torch.no_grad():\n",
    "            action = self.actor(state)\n",
    "        return action.cpu().numpy()[0]\n",
    "\n",
    "    @abstractmethod\n",
    "    def is_update(self, steps):\n",
    "        \"\"\" 現在のトータルのステップ数(steps)を受け取り，アルゴリズムを学習するか否かを返す． \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def step(self, env, state, t, steps):\n",
    "        \"\"\" 環境(env)，現在の状態(state)，現在のエピソードのステップ数(t)，今までのトータルのステップ数(steps)を\n",
    "            受け取り，リプレイバッファへの保存などの処理を行い，状態・エピソードのステップ数を更新する．\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def update(self):\n",
    "        \"\"\" 1回分の学習を行う． \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dedicated-project",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SACActor(nn.Module):\n",
    "\n",
    "    def __init__(self, state_shape, action_shape):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_shape[0], 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 2 * action_shape[0]),\n",
    "        )\n",
    "\n",
    "    def forward(self, states):\n",
    "        means, log_stds = self.net(states).chunk(2, dim=-1)\n",
    "        return torch.tanh(means)\n",
    "\n",
    "    def sample(self, states):\n",
    "        # [演習] 確率論的な行動と確率密度の対数を計算し，返します．\n",
    "        # means, log_stds = ...\n",
    "        # log_stds = log_stds.clamp(-20, 2)\n",
    "        # return ...\n",
    "        means, log_stds = self.net(states).chunk(2, dim=-1)\n",
    "        log_stds = log_stds.clamp(-20, 2)\n",
    "        return reparameterize(means, log_stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "olive-collection",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SACCritic(nn.Module):\n",
    "\n",
    "    def __init__(self, state_shape, action_shape):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net1 = nn.Sequential(\n",
    "            nn.Linear(state_shape[0] + action_shape[0], 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "        self.net2 = nn.Sequential(\n",
    "            nn.Linear(state_shape[0] + action_shape[0], 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, states, actions):\n",
    "        # [演習] ソフト状態行動価値を2つ計算し，返します．\n",
    "        # (例)\n",
    "        # return ..., ...\n",
    "        sa = torch.cat([states, actions], dim=-1)\n",
    "        q1 = self.net1(sa)\n",
    "        q2 = self.net2(sa)\n",
    "        return q1, q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "employed-czech",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "\n",
    "    def __init__(self, buffer_size, state_shape, action_shape, device):\n",
    "        # 次にデータを挿入するインデックス．\n",
    "        self._p = 0\n",
    "        # データ数．\n",
    "        self._n = 0\n",
    "        # リプレイバッファのサイズ．\n",
    "        self.buffer_size = buffer_size\n",
    "\n",
    "        # GPU上に保存するデータ．\n",
    "        self.states = torch.empty((buffer_size, *state_shape), dtype=torch.float, device=device)\n",
    "        self.actions = torch.empty((buffer_size, *action_shape), dtype=torch.float, device=device)\n",
    "        self.rewards = torch.empty((buffer_size, 1), dtype=torch.float, device=device)\n",
    "        self.dones = torch.empty((buffer_size, 1), dtype=torch.float, device=device)\n",
    "        self.next_states = torch.empty((buffer_size, *state_shape), dtype=torch.float, device=device)\n",
    "\n",
    "    def append(self, state, action, reward, done, next_state):\n",
    "        self.states[self._p].copy_(torch.from_numpy(state))\n",
    "        self.actions[self._p].copy_(torch.from_numpy(action))\n",
    "        self.rewards[self._p] = float(reward)\n",
    "        self.dones[self._p] = float(done)\n",
    "        self.next_states[self._p].copy_(torch.from_numpy(next_state))\n",
    "\n",
    "        self._p = (self._p + 1) % self.buffer_size\n",
    "        self._n = min(self._n + 1, self.buffer_size)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        idxes = np.random.randint(low=0, high=self._n, size=batch_size)\n",
    "        return (\n",
    "            self.states[idxes],\n",
    "            self.actions[idxes],\n",
    "            self.rewards[idxes],\n",
    "            self.dones[idxes],\n",
    "            self.next_states[idxes]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "rough-watson",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAC(Algorithm):\n",
    "\n",
    "    def __init__(self, state_shape, action_shape, device=torch.device('cpu'), seed=0,\n",
    "                 batch_size=256, gamma=0.99, lr_actor=3e-4, lr_critic=3e-4,\n",
    "                 replay_size=10**5, start_steps=10**4, tau=5e-3, alpha=0.2, reward_scale=1.0):\n",
    "        super().__init__()\n",
    "\n",
    "        # シードを設定する．\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "        # リプレイバッファ．\n",
    "        self.buffer = ReplayBuffer(\n",
    "            buffer_size=replay_size,\n",
    "            state_shape=state_shape,\n",
    "            action_shape=action_shape,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "        # Actor-Criticのネットワークを構築する．\n",
    "        self.actor = SACActor(\n",
    "            state_shape=state_shape,\n",
    "            action_shape=action_shape\n",
    "        ).to(device)\n",
    "        self.critic = SACCritic(\n",
    "            state_shape=state_shape,\n",
    "            action_shape=action_shape\n",
    "        ).to(device)\n",
    "        self.critic_target = SACCritic(\n",
    "            state_shape=state_shape,\n",
    "            action_shape=action_shape\n",
    "        ).to(device).eval()\n",
    "\n",
    "        # ターゲットネットワークの重みを初期化し，勾配計算を無効にする．\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "        for param in self.critic_target.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # オプティマイザ．\n",
    "        self.optim_actor = torch.optim.Adam(self.actor.parameters(), lr=lr_actor)\n",
    "        self.optim_critic = torch.optim.Adam(self.critic.parameters(), lr=lr_critic)\n",
    "\n",
    "        # その他パラメータ．\n",
    "        self.learning_steps = 0\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        self.gamma = gamma\n",
    "        self.start_steps = start_steps\n",
    "        self.tau = tau\n",
    "        self.alpha = alpha\n",
    "        self.reward_scale = reward_scale\n",
    "\n",
    "    def is_update(self, steps):\n",
    "        # 学習初期の一定期間(start_steps)は学習しない．\n",
    "        return steps >= max(self.start_steps, self.batch_size)\n",
    "\n",
    "    def step(self, env, state, t, steps):\n",
    "        t += 1\n",
    "\n",
    "        # 学習初期の一定期間(start_steps)は，ランダムに行動して多様なデータの収集を促進する．\n",
    "        if steps <= self.start_steps:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action, _ = self.explore(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = fix_state_dim(next_state)\n",
    "        \n",
    "        # not ignore done\n",
    "\n",
    "        # リプレイバッファにデータを追加する．\n",
    "        self.buffer.append(state, action, reward, done, next_state)\n",
    "\n",
    "        # エピソードが終了した場合には，環境をリセットする．\n",
    "        if done:\n",
    "            t = 0\n",
    "            next_state = env.reset()\n",
    "            next_state = fix_state_dim(next_state)\n",
    "\n",
    "        return next_state, t\n",
    "\n",
    "    def update(self):\n",
    "        self.learning_steps += 1\n",
    "        states, actions, rewards, dones, next_states = self.buffer.sample(self.batch_size)\n",
    "\n",
    "        self.update_critic(states, actions, rewards, dones, next_states)\n",
    "        self.update_actor(states)\n",
    "        self.update_target()\n",
    "\n",
    "    def update_critic(self, states, actions, rewards, dones, next_states):\n",
    "        # [演習] 現在のソフト状態行動価値とそのターゲットを計算し，ネットワークの損失関数を完成させましょう．\n",
    "        # (例)\n",
    "        # 現在のソフト状態行動価値を計算する．\n",
    "        curr_qs1, curr_qs2 = self.critic(states, actions)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # ソフト状態価値のターゲットを計算します．\n",
    "            # target_vs = ...\n",
    "            next_actions, log_pis = self.actor.sample(next_states)\n",
    "            qs1, qs2 = self.critic_target(next_states, next_actions)\n",
    "            target_vs = torch.min(qs1, qs2) - self.alpha * log_pis\n",
    "\n",
    "        # ソフト状態行動価値のターゲットを計算します．\n",
    "        # target_qs = ...\n",
    "        target_qs = rewards * self.reward_scale + (1 - dones) * self.gamma * target_vs\n",
    "\n",
    "        loss_critic1 = (curr_qs1 - target_qs).pow_(2).mean()\n",
    "        loss_critic2 = (curr_qs2 - target_qs).pow_(2).mean()\n",
    "\n",
    "        self.optim_critic.zero_grad()\n",
    "        (loss_critic1 + loss_critic2).backward(retain_graph=False)\n",
    "        self.optim_critic.step()\n",
    "\n",
    "    def update_actor(self, states):\n",
    "        actions, log_pis = self.actor.sample(states)\n",
    "        qs1, qs2 = self.critic(states, actions)\n",
    "        loss_actor = (self.alpha * log_pis - torch.min(qs1, qs2)).mean()\n",
    "\n",
    "        self.optim_actor.zero_grad()\n",
    "        loss_actor.backward(retain_graph=False)\n",
    "        self.optim_actor.step()\n",
    "\n",
    "    def update_target(self):\n",
    "        for t, s in zip(self.critic_target.parameters(), self.critic.parameters()):\n",
    "            t.data.mul_(1.0 - self.tau)\n",
    "            t.data.add_(self.tau * s.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "yellow-pricing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting new Robot Server | Tentative 1\n",
      "<class 'server_manager_pb2.RobotServer'>\n",
      "True \n",
      "Successfully started Robot Server at localhost:41693\n"
     ]
    }
   ],
   "source": [
    "ENV_ID = 'CubeRoomSearchLikeContinuously-v0'\n",
    "target_machine_ip = 'localhost'\n",
    "SEED = 0\n",
    "REWARD_SCALE = 1.0\n",
    "NUM_STEPS = 5 * 10 ** 4\n",
    "EVAL_INTERVAL = 10 ** 3\n",
    "\n",
    "env = gym.make(ENV_ID, ip=target_machine_ip, gui=True, max_episode_steps=5)\n",
    "# env_test = gym.make(ENV_ID, ip=target_machine_ip, gui=True)\n",
    "\n",
    "s_shape = (env.observation_space['agent_pose'].low.size + env.observation_space['occupancy_grid'].low.size, )\n",
    "\n",
    "algo = SAC(\n",
    "    state_shape=s_shape,\n",
    "    action_shape=env.action_space.shape,\n",
    "    seed=SEED,\n",
    "    replay_size=10**4,\n",
    "    reward_scale=REWARD_SCALE,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    env=env,\n",
    "    env_test=env,\n",
    "    algo=algo,\n",
    "    seed=SEED,\n",
    "    num_steps=NUM_STEPS,\n",
    "    eval_interval=EVAL_INTERVAL,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "median-trouble",
   "metadata": {},
   "outputs": [
    {
     "ename": "_InactiveRpcError",
     "evalue": "<_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.DEADLINE_EXCEEDED\n\tdetails = \"Deadline Exceeded\"\n\tdebug_error_string = \"{\"created\":\"@1616755993.898195400\",\"description\":\"Error received from peer ipv4:127.0.0.1:41693\",\"file\":\"src/core/lib/surface/call.cc\",\"file_line\":1063,\"grpc_message\":\"Deadline Exceeded\",\"grpc_status\":4}\"\n>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-3435b262f1ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-57b826d4d14a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;31m# 環境(self.env)，現在の状態(state)，現在のエピソードのステップ数(t)，今までのトータルのステップ数(steps)を\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;31m# アルゴリズムに渡し，状態・エピソードのステップ数を更新する．\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;31m# アルゴリズムが準備できていれば，1回学習を行う．\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-b5fab96ed4e5>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, env, state, t, steps)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfix_state_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/robo_gym/envs/mir_nav/mir_nav.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;31m# Send action to Robot Server\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrs_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRobotServerError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"send_action\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/robo_gym_server_modules/robot_server/client.py\u001b[0m in \u001b[0;36msend_action\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msend_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrobot_server_stub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSendAction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrobot_server_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuccess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/grpc/_channel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m    921\u001b[0m         state, call, = self._blocking(request, timeout, metadata, credentials,\n\u001b[1;32m    922\u001b[0m                                       wait_for_ready, compression)\n\u001b[0;32m--> 923\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_end_unary_response_blocking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    924\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m     def with_call(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/grpc/_channel.py\u001b[0m in \u001b[0;36m_end_unary_response_blocking\u001b[0;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[1;32m    824\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 826\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0m_InactiveRpcError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    827\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.DEADLINE_EXCEEDED\n\tdetails = \"Deadline Exceeded\"\n\tdebug_error_string = \"{\"created\":\"@1616755993.898195400\",\"description\":\"Error received from peer ipv4:127.0.0.1:41693\",\"file\":\"src/core/lib/surface/call.cc\",\"file_line\":1063,\"grpc_message\":\"Deadline Exceeded\",\"grpc_status\":4}\"\n>"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "devoted-korea",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parental-participation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
