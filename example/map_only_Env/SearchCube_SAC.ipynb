{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import gym\r\n",
    "from gym import spaces\r\n",
    "import robo_gym\r\n",
    "from robo_gym.wrappers.exception_handling import ExceptionHandling\r\n",
    "import numpy as np\r\n",
    "import pfrl\r\n",
    "import torch\r\n",
    "from torch import distributions, nn\r\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class WrapPyTorch(gym.ObservationWrapper):\r\n",
    "    def __init__(self, env=None):\r\n",
    "        super(WrapPyTorch, self).__init__(env)\r\n",
    "        self.observation_space = spaces.Box(low=-1, high=100, shape=(1,env.map_size,env.map_size,), dtype=np.float32)\r\n",
    "        \r\n",
    "    def observation(self, observation):\r\n",
    "        return np.expand_dims(observation, axis=0)\r\n",
    "\r\n",
    "    def reset(self, **kwargs):\r\n",
    "        return self.observation(self.env.reset(**kwargs))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "target_machine_ip = 'robot-server' # or other machine 'xxx.xxx.xxx.xxx'\r\n",
    "\r\n",
    "# initialize environment\r\n",
    "env = gym.make('CubeSearchInCubeRoomObsMapOnly-v0', ip=target_machine_ip, gui=True)\r\n",
    "env = ExceptionHandling(env)\r\n",
    "env = WrapPyTorch(env)\r\n",
    "\r\n",
    "state = env.reset(\r\n",
    "    new_room=True, \r\n",
    "    new_agent_pose=True, \r\n",
    "    obstacle_count=40,\r\n",
    "    room_length_max=15.0, \r\n",
    "    room_mass_min=150.0, \r\n",
    "    room_mass_max=160.0, \r\n",
    ")\r\n",
    "print(state)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "timestep_limit = env.spec.max_episode_steps\r\n",
    "obs_space = env.observation_space\r\n",
    "action_space = env.action_space\r\n",
    "obs_size = obs_space.low.size\r\n",
    "action_size = action_space.low.size\r\n",
    "\r\n",
    "print(f'timelimit: \\t{timestep_limit}')\r\n",
    "print(f'obs_space: \\t{obs_space} \\naction_space: \\t{action_space}')\r\n",
    "print(f'obs_size: \\t{obs_size}')\r\n",
    "print(f'action_size: \\t{action_size}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def conv2d_size_out(size, kernel_size=5, stride=2):\r\n",
    "    return (size - (kernel_size - 1) - 1) // stride + 1\r\n",
    "        \r\n",
    "def make_conv2d_layer(width, height):\r\n",
    "    convW = conv2d_size_out(width, 4, 4) # 128 -> 32\r\n",
    "    convW = conv2d_size_out(convW, 4, 4) # 32 -> 8\r\n",
    "    convW = conv2d_size_out(convW, 3, 1) # 8 -> 6\r\n",
    "\r\n",
    "    convH = conv2d_size_out(height, 4, 4)\r\n",
    "    convH = conv2d_size_out(convH, 4, 4)\r\n",
    "    convH = conv2d_size_out(convH, 3, 1)\r\n",
    "\r\n",
    "    linear_input_size = convW * convH * 64\r\n",
    "    print('size:', linear_input_size)\r\n",
    "\r\n",
    "    # RGB Image tensor as input\r\n",
    "    return nn.Sequential(\r\n",
    "        nn.Conv2d(1, 32, kernel_size=4,stride=4),\r\n",
    "        nn.ELU(),\r\n",
    "        nn.Conv2d(32, 64, kernel_size=4, stride=4),\r\n",
    "        nn.ELU(),\r\n",
    "        nn.Conv2d(64, 64, kernel_size=3,stride=1),\r\n",
    "        nn.ELU(),\r\n",
    "        nn.Flatten(),\r\n",
    "    ), linear_input_size\r\n",
    "\r\n",
    "def make_linear_layer(linear_input_size, out_size):\r\n",
    "    return nn.Sequential(\r\n",
    "        nn.Linear(linear_input_size, 256),\r\n",
    "        nn.ReLU(),\r\n",
    "        nn.Linear(256, 256),\r\n",
    "        nn.ReLU(),\r\n",
    "        nn.Linear(256, out_size),\r\n",
    "    )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def squashed_diagonal_gaussian_head(x):\r\n",
    "    assert x.shape[-1] == action_size * 2\r\n",
    "    mean, log_scale = torch.chunk(x, 2, dim=1)\r\n",
    "    log_scale = torch.clamp(log_scale, -20.0, 2.0)\r\n",
    "    var = torch.exp(log_scale * 2)\r\n",
    "    base_distribution = distributions.Independent(\r\n",
    "        distributions.Normal(loc=mean, scale=torch.sqrt(var)), 1\r\n",
    "    )\r\n",
    "    # cache_size=1 is required for numerical stability\r\n",
    "    return distributions.transformed_distribution.TransformedDistribution(\r\n",
    "        base_distribution, [distributions.transforms.TanhTransform(cache_size=1)]\r\n",
    "    )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class PolicyFunction(nn.Module):\r\n",
    "    def __init__(self, width, height, action_size):\r\n",
    "        super().__init__()\r\n",
    "\r\n",
    "        # RGB Image tensor as input\r\n",
    "        self.selectTrackFeatures, self.linear_input_size = make_conv2d_layer(width, height)\r\n",
    "        self.fc1 = make_linear_layer(self.linear_input_size, action_size*2)\r\n",
    "    \r\n",
    "    def forward(self, state):\r\n",
    "        x = self.selectTrackFeatures(state)\r\n",
    "        x = self.fc1(x)\r\n",
    "        return squashed_diagonal_gaussian_head(x)\r\n",
    "\r\n",
    "obs_map_shape = obs_space.low.shape\r\n",
    "print(obs_map_shape)\r\n",
    "policy = PolicyFunction(obs_map_shape[0], obs_map_shape[1], action_size)\r\n",
    "policy_optimizer = torch.optim.Adam(policy.parameters(), lr=3e-4)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class QFunction(nn.Module):\r\n",
    "    def __init__(self, width, height, action_size):\r\n",
    "        super().__init__()\r\n",
    "\r\n",
    "        # RGB Image tensor as input\r\n",
    "        self.selectTrackFeatures, self.linear_input_size = make_conv2d_layer(width, height)\r\n",
    "        self.fc1 = make_linear_layer(self.linear_input_size + action_size, 1)\r\n",
    "    \r\n",
    "    def forward(self, state_and_action):\r\n",
    "        state = state_and_action[0]\r\n",
    "        occupancy_vector = self.selectTrackFeatures(state)\r\n",
    "        x = torch.cat((occupancy_vector, state_and_action[1]), axis=-1)\r\n",
    "        return self.fc1(x)\r\n",
    "\r\n",
    "q_func1 = QFunction(obs_map_shape[0], obs_map_shape[1], action_size)\r\n",
    "q_func2 = QFunction(obs_map_shape[0], obs_map_shape[1], action_size)\r\n",
    "q_func1_optimizer = torch.optim.Adam(q_func1.parameters(), lr=3e-4)\r\n",
    "q_func2_optimizer = torch.optim.Adam(q_func2.parameters(), lr=3e-4)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "rbuf = pfrl.replay_buffers.ReplayBuffer(10 ** 6)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def burnin_action_func():\r\n",
    "    \"\"\"Select random actions until model is updated one or more times.\"\"\"\r\n",
    "    return np.random.uniform(action_space.low, action_space.high).astype(np.float32)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "gamma = 0.99\r\n",
    "replay_start_size = 10000\r\n",
    "gpu = 0\r\n",
    "batch_size = 256\r\n",
    "entropy_target = -action_size\r\n",
    "temperature_optimizer_lr = 3e-4\r\n",
    "\r\n",
    "agent = pfrl.agents.SoftActorCritic(\r\n",
    "    policy,\r\n",
    "    q_func1,\r\n",
    "    q_func2,\r\n",
    "    policy_optimizer,\r\n",
    "    q_func1_optimizer,\r\n",
    "    q_func2_optimizer,\r\n",
    "    rbuf,\r\n",
    "    gamma=gamma,\r\n",
    "    replay_start_size=replay_start_size,\r\n",
    "    gpu=gpu,\r\n",
    "    minibatch_size=batch_size,\r\n",
    "    burnin_action_func=burnin_action_func,\r\n",
    "    entropy_target=entropy_target,\r\n",
    "    temperature_optimizer_lr=temperature_optimizer_lr,\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import logging\r\n",
    "logger = logging.getLogger()\r\n",
    "logger.setLevel(logging.INFO)\r\n",
    "logging.info(\"Start training\")\r\n",
    "\r\n",
    "n_episodes = 1000\r\n",
    "max_episode_len = 50\r\n",
    "eval_step = 10\r\n",
    "total_R = 0\r\n",
    "\r\n",
    "for i in range(1, n_episodes + 1):\r\n",
    "    obs = env.reset(new_room=False, new_agent_pose=True)\r\n",
    "    R = 0  # return (sum of rewards)\r\n",
    "    t = 0  # time step\r\n",
    "    while True:\r\n",
    "        # Uncomment to watch the behavior in a GUI window\r\n",
    "        # env.render()\r\n",
    "        action = agent.act(obs)\r\n",
    "        obs, reward, done, _ = env.step(action)\r\n",
    "        R += reward\r\n",
    "        t += 1\r\n",
    "        reset = t == max_episode_len\r\n",
    "        agent.observe(obs, reward, done, reset)\r\n",
    "        # print(f\"action: {action}, reward: {reward}\")\r\n",
    "        if done or reset:\r\n",
    "            break\r\n",
    "            \r\n",
    "    total_R += R\r\n",
    "    if i % eval_step == 0:\r\n",
    "        logging.info(f'episode: {i}, R_mean: {total_R/eval_step} \\nstatistics: {agent.get_statistics()}')\r\n",
    "        total_R = 0\r\n",
    "\r\n",
    "logging.info('Finished')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ebb3adfefa432171f637de14e6e9f48792899ee912c9c2052085ea5d30b7a647"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit ('.venv': poetry)",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}