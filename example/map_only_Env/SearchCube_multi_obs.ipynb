{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "import gym\r\n",
    "from gym import spaces\r\n",
    "import robo_gym\r\n",
    "from robo_gym.wrappers.exception_handling import ExceptionHandling\r\n",
    "import numpy as np\r\n",
    "import pfrl\r\n",
    "import torch\r\n",
    "from torch import distributions, nn\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from collections import deque"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "class WrapPyTorch(gym.ObservationWrapper):\r\n",
    "    def __init__(self, env=None):\r\n",
    "        super(WrapPyTorch, self).__init__(env)\r\n",
    "        self.prior_count = 2\r\n",
    "        self.prior_stack = deque(maxlen=self.prior_count)\r\n",
    "        self.observation_space = spaces.Box(low=-1, high=100, shape=(self.prior_count, env.map_size, env.map_size,), dtype=np.float32)\r\n",
    "        \r\n",
    "    def observation(self, observation):\r\n",
    "        if len(self.prior_stack) == self.prior_count:\r\n",
    "            self.prior_stack.append(observation)\r\n",
    "        else:\r\n",
    "            self.prior_stack.extend([observation for _ in range(self.prior_count)])\r\n",
    "        return np.array(self.prior_stack)\r\n",
    "\r\n",
    "    def reset(self, **kwargs):\r\n",
    "        self.prior_stack.clear()\r\n",
    "        return self.observation(self.env.reset(**kwargs))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "target_machine_ip = 'robot-server' # or other machine 'xxx.xxx.xxx.xxx'\r\n",
    "\r\n",
    "# initialize environment\r\n",
    "env = gym.make('CubeSearchInCubeRoomObsMapOnly-v0', ip=target_machine_ip, gui=True, gazebo_gui=True)\r\n",
    "env = ExceptionHandling(env)\r\n",
    "env = WrapPyTorch(env)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "state = env.reset(\r\n",
    "    new_room=True,\r\n",
    "    new_agent_pose=True,\r\n",
    "    obstacle_count=32,\r\n",
    "    room_length_max=12.0,\r\n",
    "    room_mass_min=110.0,\r\n",
    "    room_mass_max=150.0,\r\n",
    ")\r\n",
    "print(state)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "timestep_limit = env.spec.max_episode_steps\r\n",
    "obs_space = env.observation_space\r\n",
    "action_space = env.action_space\r\n",
    "obs_size = obs_space.low.size\r\n",
    "action_size = action_space.low.size\r\n",
    "prior_observation_count = env.prior_count\r\n",
    "\r\n",
    "print(f'timelimit: \\t{timestep_limit}')\r\n",
    "print(f'obs_space: \\t{obs_space} \\naction_space: \\t{action_space}')\r\n",
    "print(f'obs_size: \\t{obs_size}')\r\n",
    "print(f'action_size: \\t{action_size}')\r\n",
    "print(f'prior_observation_count: \\t{prior_observation_count}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "def conv2d_size_out(size, kernel_size=5, stride=2):\r\n",
    "    return (size - (kernel_size - 1) - 1) // stride + 1\r\n",
    "        \r\n",
    "def make_conv2d_layer(channel, width, height):\r\n",
    "    convW = conv2d_size_out(width, 8, 4) # 128 -> 31\r\n",
    "    convW = conv2d_size_out(convW, 4, 3) # 31 -> 10\r\n",
    "    convW = conv2d_size_out(convW, 4, 3) # 10 -> 3\r\n",
    "\r\n",
    "    convH = conv2d_size_out(height, 8, 4)\r\n",
    "    convH = conv2d_size_out(convH, 4, 3)\r\n",
    "    convH = conv2d_size_out(convH, 4, 3)\r\n",
    "\r\n",
    "    linear_input_size = convW * convH * 64\r\n",
    "    print('size:', linear_input_size)\r\n",
    "\r\n",
    "    # RGB Image tensor as input\r\n",
    "    net = nn.Sequential(\r\n",
    "        nn.Conv2d(channel, 32, kernel_size=8,stride=4),\r\n",
    "        nn.ELU(),\r\n",
    "        nn.Conv2d(32, 64, kernel_size=4, stride=3),\r\n",
    "        nn.ELU(),\r\n",
    "        nn.Conv2d(64, 64, kernel_size=4,stride=3),\r\n",
    "        nn.ELU(),\r\n",
    "        nn.Flatten(),\r\n",
    "    )\r\n",
    "    nn.init.xavier_uniform_(net[0].weight, gain=torch.nn.init.calculate_gain('relu'))\r\n",
    "    nn.init.xavier_uniform_(net[2].weight, gain=torch.nn.init.calculate_gain('relu'))\r\n",
    "    nn.init.xavier_uniform_(net[4].weight, gain=torch.nn.init.calculate_gain('relu'))\r\n",
    "    return net, linear_input_size\r\n",
    "\r\n",
    "def make_linear_layer(linear_input_size, out_size):\r\n",
    "    net = nn.Sequential(\r\n",
    "        nn.Linear(linear_input_size, 256),\r\n",
    "        nn.ReLU(),\r\n",
    "        nn.Linear(256, 256),\r\n",
    "        nn.ReLU(),\r\n",
    "        nn.Linear(256, out_size),\r\n",
    "    )\r\n",
    "    torch.nn.init.xavier_uniform_(net[0].weight, gain=torch.nn.init.calculate_gain('relu'))\r\n",
    "    torch.nn.init.xavier_uniform_(net[2].weight, gain=torch.nn.init.calculate_gain('relu'))\r\n",
    "    torch.nn.init.xavier_uniform_(net[4].weight, gain=torch.nn.init.calculate_gain('relu'))\r\n",
    "    return net"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "def squashed_diagonal_gaussian_head(x):\r\n",
    "    assert x.shape[-1] == action_size * 2\r\n",
    "    mean, log_scale = torch.chunk(x, 2, dim=1)\r\n",
    "    log_scale = torch.clamp(log_scale, -20.0, 2.0)\r\n",
    "    var = torch.exp(log_scale * 2)\r\n",
    "    base_distribution = distributions.Independent(\r\n",
    "        distributions.Normal(loc=mean, scale=torch.sqrt(var)), 1\r\n",
    "    )\r\n",
    "    # cache_size=1 is required for numerical stability\r\n",
    "    return distributions.transformed_distribution.TransformedDistribution(\r\n",
    "        base_distribution, [distributions.transforms.TanhTransform(cache_size=1)]\r\n",
    "    )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class PolicyFunction(nn.Module):\r\n",
    "    def __init__(self, channel, width, height, action_size):\r\n",
    "        super().__init__()\r\n",
    "        # RGB Image tensor as input\r\n",
    "        self.selectTrackFeatures, self.linear_input_size = make_conv2d_layer(channel, width, height)\r\n",
    "        self.fc1 = make_linear_layer(self.linear_input_size, action_size*2)\r\n",
    "    \r\n",
    "    def forward(self, state):\r\n",
    "        x = self.selectTrackFeatures(state)\r\n",
    "        x = self.fc1(x)\r\n",
    "        return squashed_diagonal_gaussian_head(x)\r\n",
    "\r\n",
    "obs_map_shape = obs_space.low.shape\r\n",
    "print(obs_map_shape)\r\n",
    "policy = PolicyFunction(prior_observation_count, obs_map_shape[1], obs_map_shape[2], action_size)\r\n",
    "policy_optimizer = torch.optim.Adam(policy.parameters(), lr=3e-4)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class QFunction(nn.Module):\r\n",
    "    def __init__(self, channel, width, height, action_size):\r\n",
    "        super().__init__()\r\n",
    "\r\n",
    "        # RGB Image tensor as input\r\n",
    "        self.selectTrackFeatures, self.linear_input_size = make_conv2d_layer(channel, width, height)\r\n",
    "        self.fc1 = make_linear_layer(self.linear_input_size + action_size, 1)\r\n",
    "    \r\n",
    "    def forward(self, state_and_action):\r\n",
    "        state = state_and_action[0]\r\n",
    "        occupancy_vector = self.selectTrackFeatures(state)\r\n",
    "        x = torch.cat((occupancy_vector, state_and_action[1]), axis=-1)\r\n",
    "        return self.fc1(x)\r\n",
    "\r\n",
    "q_func1 = QFunction(prior_observation_count, obs_map_shape[1], obs_map_shape[2], action_size)\r\n",
    "q_func2 = QFunction(prior_observation_count, obs_map_shape[1], obs_map_shape[2], action_size)\r\n",
    "q_func1_optimizer = torch.optim.Adam(q_func1.parameters(), lr=3e-4)\r\n",
    "q_func2_optimizer = torch.optim.Adam(q_func2.parameters(), lr=3e-4)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "source": [
    "rbuf = pfrl.replay_buffers.ReplayBuffer(10 ** 6)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "source": [
    "def burnin_action_func():\r\n",
    "    \"\"\"Select random actions until model is updated one or more times.\"\"\"\r\n",
    "    return np.random.uniform(action_space.low, action_space.high).astype(np.float32)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "gamma = 0.99\r\n",
    "replay_start_size = 10000\r\n",
    "gpu = 0\r\n",
    "batch_size = 256\r\n",
    "entropy_target = -action_size\r\n",
    "temperature_optimizer_lr = 3e-4\r\n",
    "\r\n",
    "agent = pfrl.agents.SoftActorCritic(\r\n",
    "    policy,\r\n",
    "    q_func1,\r\n",
    "    q_func2,\r\n",
    "    policy_optimizer,\r\n",
    "    q_func1_optimizer,\r\n",
    "    q_func2_optimizer,\r\n",
    "    rbuf,\r\n",
    "    gamma=gamma,\r\n",
    "    replay_start_size=replay_start_size,\r\n",
    "    gpu=gpu,\r\n",
    "    minibatch_size=batch_size,\r\n",
    "    burnin_action_func=burnin_action_func,\r\n",
    "    entropy_target=entropy_target,\r\n",
    "    temperature_optimizer_lr=temperature_optimizer_lr,\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from fastprogress import master_bar, progress_bar\r\n",
    "\r\n",
    "n_episodes = 1000\r\n",
    "max_episode_len = 25\r\n",
    "eval_step = 10\r\n",
    "total_R = 0\r\n",
    "\r\n",
    "reward_traj = []\r\n",
    "mb = master_bar(range(n_episodes))\r\n",
    "\r\n",
    "for i in mb:\r\n",
    "    obs = env.reset(new_room=False, new_agent_pose=False)\r\n",
    "    R = 0  # return (sum of rewards)\r\n",
    "    t = 0  # time step\r\n",
    "    while True:\r\n",
    "        # Uncomment to watch the behavior in a GUI window\r\n",
    "        # env.render()\r\n",
    "        action = agent.act(obs)\r\n",
    "        obs, reward, done, _ = env.step(action)\r\n",
    "        R += reward\r\n",
    "        t += 1\r\n",
    "        reset = t == max_episode_len\r\n",
    "        agent.observe(obs, reward, done, reset)\r\n",
    "        # print(f\"action: {action}, reward: {reward}\")\r\n",
    "        if done or reset:\r\n",
    "            break\r\n",
    "            \r\n",
    "    x = np.arange(i+1)\r\n",
    "    reward_traj.append(R)\r\n",
    "    mb.update_graph([[x,reward_traj]])\r\n",
    "            \r\n",
    "    total_R += R\r\n",
    "    if i % eval_step == 0:\r\n",
    "        with open('train.txt', 'a') as f:\r\n",
    "            f.write(f'episode: {i}, R_mean: {total_R/eval_step} \\nstatistics: {agent.get_statistics()}\\n')\r\n",
    "        total_R = 0\r\n",
    "\r\n",
    "print('Finished')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "agent.get_statistics()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Inspection"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "import logging\r\n",
    "logger = logging.getLogger()\r\n",
    "logger.setLevel(logging.INFO)\r\n",
    "\r\n",
    "eval_count = 3\r\n",
    "target_pose = env.target_pose # (n,3)\r\n",
    "obs_map_size = env.observation_space.low.size\r\n",
    "obs_map_len = int(np.sqrt(obs_map_size))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Random Policy"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "logging.info(\"Start evaluating random policy.\")\r\n",
    "\r\n",
    "rewards = np.zeros([eval_count, max_episode_len])\r\n",
    "founds = np.zeros([eval_count, env.target_num])\r\n",
    "states = []\r\n",
    "agent_trajectories = np.zeros([eval_count, max_episode_len, 3])\r\n",
    "goal_trajectories = np.zeros([eval_count, max_episode_len, 3])\r\n",
    "start_agent_poses = np.zeros([eval_count, 3])\r\n",
    "start_maps = np.zeros([eval_count, obs_map_len, obs_map_len])\r\n",
    "\r\n",
    "for e in range(eval_count):\r\n",
    "    obs = env.reset(new_room=False, new_agent_pose=False)\r\n",
    "    start_agent_poses[e] = env.agent_pose\r\n",
    "    start_maps[e] = obs[0]\r\n",
    "    done = False\r\n",
    "    step = 0\r\n",
    "    while (not done) and (step < max_episode_len):\r\n",
    "        action = env.action_space.sample()\r\n",
    "        obs, r, done, _ = env.step(action)\r\n",
    "        states.append(obs)\r\n",
    "        rewards[e,step] = r\r\n",
    "        agent_trajectories[e,step] = env.agent_pose\r\n",
    "        goal_trajectories[e,step] = env.goal_pose\r\n",
    "        step += 1\r\n",
    "        \r\n",
    "    founds[e] = env.target_found\r\n",
    "            "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "found_probs = [np.sum(founds[:,f])/eval_count for f in range(len(founds[0]))]\r\n",
    "total_reward_mean = sum([np.sum(rewards[i]) for i in range(eval_count)]) / eval_count\r\n",
    "\r\n",
    "print(f'target found probability: {found_probs}')\r\n",
    "print(f'total reward mean: {total_reward_mean}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "plt.figure(figsize=(12, 6), dpi=120)\r\n",
    "\r\n",
    "plt.subplot(1,2,1)\r\n",
    "plt.grid(True)\r\n",
    "plt.scatter(goal_trajectories[:,:,0], goal_trajectories[:,:,1], marker='+', alpha=0.3, linewidths=1, label='goal')\r\n",
    "plt.scatter(agent_trajectories[:,:,0], agent_trajectories[:,:,1], marker='+', alpha=0.3, linewidths=1,  label='agent')\r\n",
    "plt.scatter(target_pose[:,0], target_pose[:,1], c='red', marker='*', label='target')\r\n",
    "plt.legend()\r\n",
    "\r\n",
    "plt.subplot(1,2,2)\r\n",
    "plt.grid(True)\r\n",
    "plt.scatter(start_agent_poses[:,0], start_agent_poses[:,1], c='green', marker='+', label='start pose')\r\n",
    "plt.scatter(target_pose[:,0], target_pose[:,1], c='red', marker='*', label='target')\r\n",
    "plt.legend()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Trained Policy"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "logging.info(\"Start evaluating traind policy.\")\r\n",
    "\r\n",
    "t_rewards = np.zeros([eval_count, max_episode_len])\r\n",
    "t_founds = np.zeros([eval_count, env.target_num])\r\n",
    "t_states = []\r\n",
    "t_agent_trajectories = np.zeros([eval_count, max_episode_len, 3])\r\n",
    "t_goal_trajectories = np.zeros([eval_count, max_episode_len, 3])\r\n",
    "t_start_agent_poses = np.zeros([eval_count, 3])\r\n",
    "t_start_maps = np.zeros([eval_count, obs_map_len, obs_map_len])\r\n",
    "\r\n",
    "with agent.eval_mode():\r\n",
    "    for e in range(eval_count):\r\n",
    "        obs = env.reset(new_room=False, new_agent_pose=True)\r\n",
    "        t_start_agent_poses[e] = env.agent_pose\r\n",
    "        t_start_maps[e] = obs[0]\r\n",
    "        done = False\r\n",
    "        step = 0\r\n",
    "        while (not done) and (step < max_episode_len):\r\n",
    "            action = agent.act(obs)\r\n",
    "            obs, r, done, _ = env.step(action)\r\n",
    "            t_states.append(obs)\r\n",
    "            t_rewards[e,step] = r\r\n",
    "            t_agent_trajectories[e,step] = env.agent_pose\r\n",
    "            t_goal_trajectories[e,step] = env.goal_pose\r\n",
    "            step += 1\r\n",
    "            \r\n",
    "        logging.info(f'current episode\\'s total reward: {np.sum(t_rewards[e])}')\r\n",
    "        t_founds[e] = env.target_found"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "t_found_probs = [np.sum(t_founds[:,f])/eval_count for f in range(len(t_founds[0]))]\r\n",
    "t_total_reward_mean = sum([np.sum(t_rewards[i]) for i in range(eval_count)]) / eval_count\r\n",
    "\r\n",
    "print(f'target found probability: {t_found_probs}')\r\n",
    "print(f'total reward mean: {t_total_reward_mean}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "plt.figure(figsize=(12, 6), dpi=120)\r\n",
    "\r\n",
    "plt.subplot(1,2,1)\r\n",
    "plt.grid(True)\r\n",
    "plt.scatter(t_goal_trajectories[:,:,0], t_goal_trajectories[:,:,1], marker='+', alpha=0.3, linewidths=1, label='goal')\r\n",
    "plt.scatter(t_agent_trajectories[:,:,0], t_agent_trajectories[:,:,1], marker='+', alpha=0.3, linewidths=1,  label='agent')\r\n",
    "plt.scatter(target_pose[:,0], target_pose[:,1], c='red', marker='*', label='target')\r\n",
    "plt.legend()\r\n",
    "\r\n",
    "plt.subplot(1,2,2)\r\n",
    "plt.grid(True)\r\n",
    "plt.scatter(t_start_agent_poses[:,0], t_start_agent_poses[:,1], c='green', marker='+', label='start pose')\r\n",
    "plt.scatter(target_pose[:,0], target_pose[:,1], c='red', marker='*', label='target')\r\n",
    "plt.legend()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "rbuf.save('./rbuf')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "agent.save('./agent')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ebb3adfefa432171f637de14e6e9f48792899ee912c9c2052085ea5d30b7a647"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}